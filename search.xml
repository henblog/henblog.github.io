<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>大数据离线知识点</title>
      <link href="/2022/07/15/da-shu-ju-zhi-shi-dian/"/>
      <url>/2022/07/15/da-shu-ju-zhi-shi-dian/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h3 id="linux常用命令"><a href="#linux常用命令" class="headerlink" title="linux常用命令"></a>linux常用命令</h3><table><thead><tr><th>序号</th><th>命令</th><th>命令解释</th></tr></thead><tbody><tr><td>1</td><td>top &#x2F; free</td><td>查看内存</td></tr><tr><td>2</td><td>df -h</td><td>查看磁盘存储情况</td></tr><tr><td>3</td><td>iotop</td><td>查看磁盘IO读写(yum install iotop安装）</td></tr><tr><td>4</td><td>iotop -o</td><td>直接查看比较高的磁盘读写程序</td></tr><tr><td>5</td><td>netstat -tunlp | grep 端口号</td><td>查看端口占用情况</td></tr><tr><td>6</td><td>uptime</td><td>查看报告系统运行时长及平均负载</td></tr><tr><td>7</td><td>ps -ef</td><td>查看进程</td></tr></tbody></table><h4 id="Shell常用工具"><a href="#Shell常用工具" class="headerlink" title="Shell常用工具"></a>Shell常用工具</h4><p>awk{print $1}；sed；cut；sort；xargs -n</p><p>单引号不取变量值，反引号执行命令，双引号嵌套单引号取变量值</p><hr><h3 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h3><table><thead><tr><th></th><th>hadoop2.x</th><th>Hadoop3.x</th></tr></thead><tbody><tr><td>访问HDFS端口</td><td>50070</td><td>9870</td></tr><tr><td>访问MR执行情况端口</td><td>8088</td><td>8088</td></tr><tr><td>历史服务器</td><td>19888</td><td>19888</td></tr><tr><td>客户端访问集群端口</td><td>9000</td><td>8020</td></tr></tbody></table><h4 id="HDFS读流程"><a href="#HDFS读流程" class="headerlink" title="HDFS读流程"></a>HDFS读流程</h4><ol><li>客户端向NameNode发送请求</li><li>NN返回目标文件的元数据</li><li>客户端向DataNode请求读取数据</li><li>读到数据后关闭FSDataInputStream</li></ol><h4 id="HDFS写流程"><a href="#HDFS写流程" class="headerlink" title="HDFS写流程"></a>HDFS写流程</h4><ol><li>客户端向NN请求上传</li><li>NN判断是否可以上传(权限 &#x2F; 文件if exists)并响应</li><li>Cli请求上传第一个块，请返回DN</li><li>NN根据节点选择，返回DN</li><li>Cli与对应DN建立传输通道</li><li>应答成功后传输数据</li><li>完成后关闭FSDataOutputStream</li></ol><h4 id="HDFS小文件处理"><a href="#HDFS小文件处理" class="headerlink" title="HDFS小文件处理"></a>HDFS小文件处理</h4><p>影响：<br>​1、存储：一个文件占用一个文件块,文件元数据存储在nn内会影响NameNode的寿命<br>​2、计算：一个小文件启用一个MapTask,默认1G</p><p>解决：<br>​1、采用har归档方式<br>​2、文件切片，采用CombineTextInputFormat把多个文件合并到一起统一切片<br>​3、开启JVM重用</p><h4 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h4><ul><li>内存是动态分配的，最小1G,每增加100万个block，增加1G内存</li><li>NN会维护一个心跳以确定DN的存在</li></ul><h4 id="纠删码原理"><a href="#纠删码原理" class="headerlink" title="纠删码原理"></a>纠删码原理</h4><p>每3个数据单元，生成2个校验单元，这5个单元中，只要有任意的3个单元存在(不管是数据单元还是校验单元，只要总数&#x3D;3)，就可以得到原始数据。</p><h4 id="MapReduce运行流程"><a href="#MapReduce运行流程" class="headerlink" title="MapReduce运行流程"></a>MapReduce运行流程</h4><p>Cli提交job -&gt;Yarn RM -&gt; TextInputFormat -&gt; kv -&gt; Mapper -&gt; 环形缓冲区 -&gt; Partitioner -&gt; 排序 -&gt; Merge归并 -&gt;Reducer -&gt;Merge </p><h4 id="shuffle及优化"><a href="#shuffle及优化" class="headerlink" title="shuffle及优化"></a>shuffle及优化</h4><ol><li>map后，reduce前(排序-&gt;分区内有序，归约-&gt;combiner，分组)</li><li>不影响业务逻辑下采用Combiner预聚合</li><li>减少溢写次数，修改环形缓冲区大小</li><li>自定义分区，减少数据倾斜</li><li>采用压缩，减少IO</li></ol><h4 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h4><ul><li>Application:Mr申请 -&gt; RM返回路径 -&gt; Mr提交资源 -&gt; 运行AM -&gt; NodeManager -&gt;  运行 -&gt; 注销YarnChild </li><li>Yarn的调度器:FIFO &#x2F; 容量(资源紧张) &#x2F; 公平(并发度)</li></ul><h4 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h4><ol><li>提前在map将key聚合combine,减少传输</li><li>两阶段聚合(局部聚合+整体聚合：如果key分布在不同mapper:二次mr，第一次将key随机散列到不同reducer进行处理达到负载均衡目的。第二次再根据去掉key的随机前缀，按原key进行reduce处理。)</li><li>实现自定义分区</li><li>增加Reducer，提升并行度</li></ol><hr><h3 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h3><h4 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h4><p>ls &#x2F; get &#x2F; creatae &#x2F; delete</p><h4 id="CAP法则"><a href="#CAP法则" class="headerlink" title="CAP法则"></a>CAP法则</h4><p>一个分布式系统不可能同时满足以下三种：</p><ul><li>一致性(C Consistency)：数据在多个副本之间能够保持数据一致的特性；</li><li>可用性 (A Available)：系统提供的服务一直处于可用的状态；</li><li>分区容错性(P Partition Tolerance)：分布式系统在遇到任何网络分区故障时，仍然需要能够保证对外提供满足一致性和可用性的服务</li></ul><p>ZooKeeper保证的CP</p><ol><li>不能保证每次服务请求的可用性(极端情况消费者程序要重新请求)</li><li>进行Leader选举时集群不可用</li></ol><h4 id="选举机制"><a href="#选举机制" class="headerlink" title="选举机制"></a>选举机制</h4><p>半数机制：2n+1，安装奇数台</p><p>10 &#x2F; 20 &#x2F; 100台服务器–3 &#x2F; 5 &#x2F; 11台安装zookeeper</p><h5 id="第一次选举"><a href="#第一次选举" class="headerlink" title="第一次选举"></a>第一次选举</h5><ol><li>服务器启动发起选举</li><li>首先给自己来一票</li><li>如果服务器状态为LOOKING则交换选票，否则保持</li><li>myid小的改票为下一台</li><li>如果下一台服务器的票数超过半数，立即当选Leader</li><li>后来服务器发动选举，如果有Leader则服从</li></ol><h5 id="非第一次选举"><a href="#非第一次选举" class="headerlink" title="非第一次选举"></a>非第一次选举</h5><p>服务器无法与Leader保持连接：</p><ul><li>服务器试图选举，被告知存在Leader，需要建立连接并状态同步</li></ul><p>集群中不存在Leader：</p><ul><li>EPOCH(任期代号)&gt;事务id大&gt;服务器id</li></ul><h4 id="Follower和Leader状态同步"><a href="#Follower和Leader状态同步" class="headerlink" title="Follower和Leader状态同步"></a>Follower和Leader状态同步</h4><p>当Follower或Leader挂掉会进行状态同步,L和F通过心跳监测机制来感知彼此的情况。</p><p>同步策略：差异化同步(DIFF)、回滚同步(TRUNC)、全量同步(SNAP)</p><ul><li>当Follower挂掉：事务id小于Leader的事务id则差异化，大于则回滚</li><li>当Leader挂掉：以新Leader为主，保证让 followers 同步已提交的提议，丢弃未提交的提议</li></ul><hr><h3 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h3><p>海量日志采集、聚合和传输的系统</p><p><img src="/../pic/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9F%A5%E8%AF%86%E7%82%B9/1657863927209.png" alt="1657863927209"></p><h4 id="组成"><a href="#组成" class="headerlink" title="组成"></a>组成</h4><p>1.event:基本传输单元-&gt; header|body</p><p>2.taildir source:</p><ul><li>断点续传、多目录；</li><li>不支持递归文件夹读取文件–需要自定义。</li></ul><p>3.channel:</p><ul><li>file channel :数据存储磁盘，可靠性高传输效率低；(可以配置dataDirs指向多个路径，增大吞吐量)–安全</li><li>Memory channel: 数据存储于内存，速度快可靠性差；–速度</li><li>Kafka channel：数据存储在Kafka，基于磁盘，可靠性高速度快，省去了sink阶段；–面向Kafka</li></ul><p>4.HDFS sink:拉取event，送到hdfs</p><p>5.Agent：source、channel、sink组成</p><h4 id="拦截器"><a href="#拦截器" class="headerlink" title="拦截器"></a>拦截器</h4><p>ETL拦截器、时间戳拦截器、自定义拦截器(注意实现Interceptor.Builder)</p><p>拦截器可以不用，但是需要在下一级进行处理。</p><h4 id="Channel选择器"><a href="#Channel选择器" class="headerlink" title="Channel选择器"></a>Channel选择器</h4><ul><li>Replicating：默认选择器(复制)</li><li>Multiplexing：选择性发往指定通道(多路复用)</li></ul><h4 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h4><ul><li>LoadBalancingSinkProcessor 可以实现负载均衡的功能；</li><li>FailoverSinkProcessor 可以错误恢复的功能</li></ul><h4 id="Flume监控器"><a href="#Flume监控器" class="headerlink" title="Flume监控器"></a>Flume监控器</h4><p>采用Ganglia，如果监控到尝试提交的次数远远大于最终成功的次数，说明Flume运行较差，可以增加内存。</p><h4 id="Flume的事务机制"><a href="#Flume的事务机制" class="headerlink" title="Flume的事务机制"></a>Flume的事务机制</h4><ul><li>使用两个独立的事务分别负责从Source 到 Channel(put)，以及从 Channel 到 Sink(take) 的事件传递;</li><li>数据可能重复(sink发出数据没响应会继续发送)、但不会丢失(事务机制，Memory Channel可能丢)；</li></ul><hr><h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><h4 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h4><p>生产者、Broker、消费者、Zookeeper(Broker id、ids等)</p><p><img src="C:\Users\86188\AppData\Roaming\Typora\typora-user-images\1651741526444.png" alt="1651741526444"></p><h4 id="参数配置"><a href="#参数配置" class="headerlink" title="参数配置"></a>参数配置</h4><ul><li>机器数量&#x3D;2 *(峰值生产速度 * 副本数  &#x2F;  100)+ 1</li><li>副本数：2 &#x2F; 3 -&gt; 提高可靠增加IO</li><li>日志保存(7天 ，建议3天</li><li>硬盘大小&#x3D;数据量 * 副本 * 天数  &#x2F;  70%</li><li>分区数&#x3D;期望吞吐量 &#x2F; min(producer,consumer吞吐量)</li><li>Topic数量&#x3D;日志类型数量</li><li>内存：默认1g</li></ul><h4 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h4><p>面向一个topic</p><ul><li><p>发送方式：同步、异步、带回调函数异步</p></li><li><p>分区分配策略：producer可以指定分区，将数据存储在（key的hash值 % 分区数）号分区中</p></li><li><p>提高吞吐：批次大小(batch.size:16k-&gt;32k) &#x2F; 等待时间(linger.ms:0-&gt;5-100ms) &#x2F; 压缩(compression.type) &#x2F; 缓冲区大小(32m-&gt;64m)</p></li><li><p>数据可靠：</p><p>Ack:</p><ol><li>Ack&#x3D;0:生产者发送消息增加offset，继续生产；数据可能丢失</li><li>Ack&#x3D;1:Leader应答增加offset；</li><li>Ack&#x3D;-1:Leader和ISR队列所有Follwer应答增加offset；数据可能重复</li></ol><p>幂等性(enable.idempotence) :生产者发送的消息Broker端只持久化一条（单分区单会话）</p><p>精确一次&#x3D;  幂等性 +  至少一次( ( ack&#x3D;-1 +  分区副本数&gt;&#x3D;2 + ISR 最小副本数量&gt;&#x3D;2) )+无限重试</p></li><li><p>数据重复：开启事务（ 必须指定transactional.id）或在下一级（分组、开窗）</p></li></ul><h4 id="Broker"><a href="#Broker" class="headerlink" title="Broker"></a>Broker</h4><p>与zookeeper交互保证集群运行，版本0.9之后offset保存在topic中。</p><h5 id="服役新节点"><a href="#服役新节点" class="headerlink" title="服役新节点"></a>服役新节点</h5><ol><li>创建一个要均衡的主题</li><li>生成一个负载均衡的计划</li><li>创建副本存储计划</li><li>执行副本存储计划</li><li>验证副本存储计划</li></ol><h5 id="退役旧节点"><a href="#退役旧节点" class="headerlink" title="退役旧节点"></a>退役旧节点</h5><ul><li>生成执行计划，同服役</li></ul><p>Flower挂：踢出isr,其他不影响，上线后读取log里面副本中最小的offset（HW)，直到&gt;&#x3D;HW</p><p>Leader挂：从isr选新，其余follower从新Leader同步</p><h5 id="offset提交"><a href="#offset提交" class="headerlink" title="offset提交"></a>offset提交</h5><p>自动（可能重复消费） &#x2F; 手动（可能漏消费）；</p><p>为避免可以将消费过程与提交offset过程做原子绑定，要求下游支持事务。</p><h4 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h4><ul><li><p>拉取数据</p></li><li><p>Consumer Group （CG ）：消费者组，由多个 consumer组成,面向一个topic。</p><p>1.消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费，一个消费者可以消费多个分区（组内只有自己消费此分区）；</p><p>2.消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者</p></li><li><p>再平衡及Range(将TopicAndPartition按照hashCode排序，轮询发给每个消费线程)</p></li><li><p>保证数据有序：开启幂等性-&gt;缓存数量小于5个，会在服务端重新排序 ;关闭幂等性-&gt;缓存数量设为1（因幂等性，所以单分区有序）</p></li><li><p>副本同步队列：zookeeper会维持一个isr队列，Leader挂掉会顺延isr中下一个服务，当心跳超过45s或者处理消息超过5分钟就会认为服务不可用并剔除出isr。</p></li><li><p>数据可靠：手动提交offset</p></li><li><p>可以按照时间消费数据：需要将offset转换为时间戳KafkaUtil.fetchOffsetsWithTimestamp</p></li></ul><h4 id="数据积压"><a href="#数据积压" class="headerlink" title="数据积压"></a>数据积压</h4><p>消费能力不足：增加topic分区数，提升消费者数量，消费者数 &#x3D; 分区数；</p><p>下游处理不及时（生产速度&gt;拉取速度）：提高每批次拉取的数量；</p><h4 id="如何实现高效读写"><a href="#如何实现高效读写" class="headerlink" title="如何实现高效读写"></a>如何实现高效读写</h4><ol><li>本身是分布式集群，同时采用分区技术，并发度高；</li><li>顺序写磁盘：生产者生产数据是追加到log文件的末端；</li><li>读数据采用稀疏索引，可以快速定位</li><li>零复制技术</li></ol><h4 id="单条日志大小"><a href="#单条日志大小" class="headerlink" title="单条日志大小"></a>单条日志大小</h4><p>如果遇到一条消息大于1M,默认情况下可能procuder无法推送或consumer无法消费；</p><p>需要配置server.properties中的replica.fetch.max.bytes（可复制的消息的最大字节数）和message.max.bytes（kafka 会接收单个消息size的最大限制）。需注意接收必须小于等于复制</p><h4 id="过期数据清除"><a href="#过期数据清除" class="headerlink" title="过期数据清除"></a>过期数据清除</h4><p>log.cleanup.policy&#x3D;delete（删除） &#x2F;  compact（压缩）</p><h4 id="数据顺序"><a href="#数据顺序" class="headerlink" title="数据顺序"></a>数据顺序</h4><p>单分区内有序；多分区，分区与分区间无序；</p><p>生产者可以指定key以将数据发送到同一个分区；</p><hr><h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3><p>可以将结构化数据映射成一张数据表,并提供类SQL的查询功能，本质是将SQL语句转化成MapReduce程序；</p><p>Hive元数据默认存储在derby数据库，不支持多客户端访问，所以将元数据存储在MySQL，支持多客户端访问。</p><p><img src="C:\Users\86188\AppData\Roaming\Typora\typora-user-images\1651738836403.png" alt="1651738836403"></p><h4 id="与数据库比较"><a href="#与数据库比较" class="headerlink" title="与数据库比较"></a>与数据库比较</h4><ul><li>类似的查询语句；</li><li>数据存储在HDFS；</li><li>数据更新不建议改写；</li><li>执行延迟较高，需要将hql转换为Map Reduce跑job</li><li>数据规模很大；</li></ul><h4 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h4><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">CREATE</span> <span class="token punctuation">[</span>EXTERNAL<span class="token punctuation">]</span> <span class="token keyword">TABLE</span> <span class="token punctuation">[</span><span class="token keyword">IF</span> <span class="token operator">NOT</span> <span class="token keyword">EXISTS</span><span class="token punctuation">]</span> table_name <span class="token operator">/</span> <span class="token operator">*</span>创建<span class="token punctuation">[</span>外部<span class="token punctuation">]</span>表<span class="token operator">*</span> <span class="token operator">/</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>col_name data_type <span class="token punctuation">[</span><span class="token keyword">COMMENT</span> col_comment<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token keyword">COMMENT</span> table_comment<span class="token punctuation">]</span><span class="token punctuation">[</span>PARTITIONED <span class="token keyword">BY</span> <span class="token punctuation">(</span>col_name data_type <span class="token punctuation">[</span><span class="token keyword">COMMENT</span> col_comment<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token operator">*</span>分区<span class="token operator">*</span> <span class="token operator">/</span> <span class="token punctuation">[</span><span class="token keyword">CLUSTERED</span> <span class="token keyword">BY</span> <span class="token punctuation">(</span>col_name<span class="token punctuation">,</span> col_name<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token operator">*</span>分桶表<span class="token operator">*</span> <span class="token operator">/</span> <span class="token punctuation">[</span>SORTED <span class="token keyword">BY</span> <span class="token punctuation">(</span>col_name <span class="token punctuation">[</span><span class="token keyword">ASC</span><span class="token operator">|</span><span class="token keyword">DESC</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">INTO</span> num_buckets BUCKETS<span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token operator">*</span>桶内排序<span class="token operator">*</span> <span class="token operator">/</span> <span class="token punctuation">[</span><span class="token keyword">ROW</span> FORMAT row_format<span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token operator">*</span>格式化表<span class="token operator">*</span> <span class="token operator">/</span> <span class="token punctuation">[</span>STORED <span class="token keyword">AS</span> file_format<span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token operator">*</span>指定存储文件类型<span class="token operator">*</span> <span class="token operator">/</span> <span class="token punctuation">[</span>LOCATION hdfs_path<span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token operator">*</span>指定表在HDFS的路径<span class="token operator">*</span> <span class="token operator">/</span> <span class="token punctuation">[</span>TBLPROPERTIES <span class="token punctuation">(</span>property_name<span class="token operator">=</span>property_value<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token operator">*</span><span class="token operator">*</span> <span class="token operator">/</span> <span class="token punctuation">[</span><span class="token keyword">AS</span> select_statement<span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token operator">*</span>根据查询结果创建表<span class="token operator">*</span> <span class="token operator">/</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"> <span class="token operator">/</span> <span class="token operator">*</span>格式化表种类<span class="token operator">*</span> <span class="token operator">/</span> DELIMITED <span class="token punctuation">[</span><span class="token keyword">FIELDS</span> <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> <span class="token keyword">char</span><span class="token punctuation">]</span>  <span class="token operator">/</span> <span class="token operator">*</span>指定分隔符<span class="token operator">*</span> <span class="token operator">/</span> <span class="token punctuation">[</span>COLLECTION ITEMS <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> <span class="token keyword">char</span><span class="token punctuation">]</span><span class="token punctuation">[</span>MAP <span class="token keyword">KEYS</span> <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> <span class="token keyword">char</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token keyword">LINES</span> <span class="token keyword">TERMINATED</span> <span class="token keyword">BY</span> <span class="token keyword">char</span><span class="token punctuation">]</span><span class="token operator">|</span>SERDE serde_name<span class="token punctuation">[</span><span class="token keyword">WITH</span> SERDEPROPERTIES <span class="token punctuation">(</span>property_name<span class="token operator">=</span>property_value<span class="token punctuation">,</span> property_name<span class="token operator">=</span>property_value<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="内部-管理-表和外部表external"><a href="#内部-管理-表和外部表external" class="headerlink" title="内部(管理)表和外部表external"></a>内部(管理)表和外部表external</h4><p>元数据：指向原始数据的路径；原始数据：数据的存储位置</p><table><thead><tr><th></th><th>内部表</th><th>外部表</th></tr></thead><tbody><tr><td>创建</td><td>移动原始数据</td><td>路径记录到元数据</td></tr><tr><td>删除</td><td>元 &#x2F; 原始数据都删</td><td>只删除元数据</td></tr></tbody></table><h4 id="四个By"><a href="#四个By" class="headerlink" title="四个By"></a>四个By</h4><ul><li>Order By：全局排序，只有一个Reducer；</li><li>Sort By：分区内有序；</li><li>Distrbute By：根据指定字段进行分区，将数据划分到不同reduce中，结合sort by使用,distrbute by 在前（例如按学院编号分区，学生学号排序）；</li><li>Cluster By：当Distribute by和Sorts by字段相同时，可以使用Cluster by方式，只能升序；</li></ul><h4 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h4><ul><li>nvl(val,def_val):返回非空值否则null；</li><li>coalesce(v1,v2,…):返回第一个非空值否则null；</li><li>CASE WHEN THEN ELSE END ：多分支；</li><li>grouping_set:多维分析；</li><li>cast(col as type):格式转换;</li></ul><h5 id="日期函数"><a href="#日期函数" class="headerlink" title="日期函数"></a>日期函数</h5><ul><li>unix_timestamp:返回当前或指定时间的时间戳；</li><li>year &#x2F; month &#x2F; day &#x2F; hour &#x2F; minute &#x2F; second &#x2F; week:年月日时分秒周；</li><li>datediff：两个日期相差的天数；</li><li>date_add：日期加减天数；</li><li>add_months：日期加减月；</li><li>next_day(date,week): 下周几的日期；</li><li>last_day:当月最后一天；</li><li>date_format(): 格式化日期；</li></ul><h5 id="取整函数"><a href="#取整函数" class="headerlink" title="取整函数"></a>取整函数</h5><ul><li>round： 四舍五入；</li><li>ceil：  向上取整；</li><li>floor： 向下取整；</li><li>rand:随机数；</li></ul><h5 id="字符串函数"><a href="#字符串函数" class="headerlink" title="字符串函数"></a>字符串函数</h5><ul><li>upper： 转大写；</li><li>lower： 转小写；</li><li>length： 长度；</li><li>trim：  前后去空格；</li><li>reverse：反转；</li><li>concat:字符串连接；</li><li>concat_ws:带分隔符字符串连接（只接受string or array<string>）；</li><li>substr:字符串截取；</li><li>get_json_object(string json_string, string path)：解析json函数；</li><li>regexp_replace：使用正则表达式匹配目标字符串，匹配成功后替换；</li></ul><h5 id="集合操作"><a href="#集合操作" class="headerlink" title="集合操作"></a>集合操作</h5><ul><li>size： 集合中元素的个数；</li><li>spilt:按照指定字符切割返回字符串数组；</li><li>map_keys &#x2F; map_values： 返回map中的key &#x2F; value；</li><li>array_contains: 判断array中是否包含某个元素；</li><li>collect_set(col) &#x2F; collect_list() : 返回去重 &#x2F; 不去重后的汇总array (接受基本数据类型)；</li><li>explode：将array或map炸裂成多行（如果想要与原字段关联需要用到lateral view侧表）；</li><li>LATERAL VIEW udtf(expression) tableAlias AS columnAlias:udtf函数与原字段关联所需，在from表后；</li></ul><h4 id="更多"><a href="#更多" class="headerlink" title="更多"></a>更多</h4><ul><li><a href="https://blog.csdn.net/hello_java_lcl/article/details/106690120">更多</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive//LanguageManual">官网</a></li></ul><h4 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h4><h5 id="rank排名函数"><a href="#rank排名函数" class="headerlink" title="rank排名函数"></a>rank排名函数</h5><ul><li>RANK() 排序相同时会重复，总数不会变(113)；</li><li>DENSE_RANK() 排序相同时会重复，总数会减少(112)；</li><li>ROW_NUMBER() 会根据顺序计算(123)；</li></ul><h5 id="over-窗口大小"><a href="#over-窗口大小" class="headerlink" title="over()窗口大小"></a>over()窗口大小</h5><ul><li>current row:当前行；</li><li>n preceding &#x2F; following：往前 &#x2F; 后几行；</li><li>unbounded:起点，unbounded preceding：从前面起点，unbounded following：到后面终点；</li><li>LAG &#x2F; LEAD(col,n):往前 &#x2F; 后第n行数据；</li><li>ntile(n):将数据分成n块；</li></ul><h5 id="例"><a href="#例" class="headerlink" title="例"></a>例</h5><p>rank() over(partition by col1,col2 order by col1 desc,col3 asc rows between … and …)</p><h4 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h4><ul><li>UDF：一进一出；</li><li>UDAF：聚集函数，多进一出（max,rank）；</li><li>UDTF： 一进多出;<ol><li>udtf -&gt; 重写3个方法：a. initialize（自定义输出的列名和类型）b. process（将结果返回forward(result)）c. close ;</li><li>上传jar包到hive的classpath下</li><li>创建函数</li><li>使用函数</li></ol></li></ul><h4 id="DML数据操作"><a href="#DML数据操作" class="headerlink" title="DML数据操作"></a>DML数据操作</h4><h5 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h5><ul><li>load data [local] inpath ‘数据的 path’ [overwrite] into table tab_name [partition (partcol1&#x3D;val1,…)]; -&gt;装载数据</li><li>insert [overwrite|into] table tab_name partition(partcol1&#x3D;val1) &lt;查询语句&gt;；-&gt;通过查询插入数据</li><li>create table if not exists tab_name as select；-&gt;建表时加载数据</li><li>location ‘Path’ ;-&gt;建表时指定数据路径</li><li>import table tab_name from ‘Path’; -&gt;export导出后，Import导入</li></ul><h5 id="导出"><a href="#导出" class="headerlink" title="导出"></a>导出</h5><ul><li>insert overwrite [local] directory ‘Path’ &lt;查询语句&gt; ；-&gt;insert导出</li><li>Hadoop中原始数据导出到本地</li><li>hive -f &#x2F; -e 执行语句或者脚本 &gt; file ；-&gt; Linux追加到文件</li><li>export table tab_name to ‘Path’; -&gt;两个 Hadoop 平台集群之间 Hive 表迁移</li><li>Sqoop 导出</li></ul><h4 id="hive优化"><a href="#hive优化" class="headerlink" title="hive优化"></a>hive优化</h4><h5 id="explain查看执行计划"><a href="#explain查看执行计划" class="headerlink" title="explain查看执行计划"></a>explain查看执行计划</h5><p>EXPLAIN [EXTENDED…] query-sql 查看sql[详细]执行计划</p><p>show partitions|functions name 查看分区表|函数情况</p><h5 id="建表（分区-x2F-桶-存储格式-压缩）"><a href="#建表（分区-x2F-桶-存储格式-压缩）" class="headerlink" title="建表（分区 &#x2F; 桶,存储格式,压缩）"></a>建表（分区 &#x2F; 桶,存储格式,压缩）</h5><ol><li><p>分区表</p><p>对应HDFS上的独立文件夹，分区就是分目录；</p><p>分区字段不能是表中存在的数据，可以看作时表的伪列；</p><p>查询时通过where查询指定分区，效率无敌！；</p><ol><li><p>创建</p><p>建表时加入partitioned by (col string)；</p><p>分区表加载数据时，必须指定分区；</p></li><li><p>二级分区</p><p>分区字段增加一个</p></li><li><p>动态分区</p><p>对分区表insert数据时，数据库会根据分区字段将数据插入对应分区。</p><p>配置：开启动态分区功能、设置为非严格模式、在每个 &#x2F; 所有MR节点可以创建多少动态分区、MR Job中，最大可以创建多少个HDFS文件。例：</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> tab_name <span class="token keyword">partition</span><span class="token punctuation">(</span>loc<span class="token punctuation">)</span> <span class="token keyword">select</span> col1<span class="token punctuation">,</span> loc <span class="token keyword">from</span> dept<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ol></li><li><p>分桶表</p><p>分区针对的是数据的存储路径，分桶针对的是数据文件。</p><ol><li><p>创建</p><p>建表时加入 clustered by(id) into 4 buckets；</p><p>reduce个数设置-1或大于等于分桶数，</p></li><li><p>抽样查询</p><p> <code>TABLESAMPLE(BUCKET x OUT OF y)</code> ：x&lt;&#x3D;y</p></li></ol></li><li><p>文件格式</p><ol><li><p>列式存储和行式存储</p><p>行存储更适合查询一整行数据的情况；</p><p>列存储是个查询只需要少数字段情况，由于每个字段数据类型相同，所以更适合压缩；</p></li><li><p>TextFile格式</p><p>默认格式，数据不做压缩耗费资源；行式存储</p></li><li><p>Orc格式</p><p>列式存储</p></li><li><p>Parquet格式</p><p>以二进制方式存储，文件包括数据和元数据；列式存储</p></li></ol></li><li><p>压缩格式</p><table><thead><tr><th>压缩格式</th><th>切片</th></tr></thead><tbody><tr><td>DEFLATE</td><td>否</td></tr><tr><td>Gzip</td><td>否</td></tr><tr><td>bzip2</td><td>是</td></tr><tr><td>LZO</td><td>是</td></tr><tr><td>Snappy</td><td>否</td></tr></tbody></table></li></ol><h5 id="语法优化"><a href="#语法优化" class="headerlink" title="语法优化"></a>语法优化</h5><ul><li><p>多重插入</p><p>如果导入数据时遇到多条sql模式相同并且从同一个表扫描，可以使用多重插入，一次读取，多次插入。</p></li><li><p>行列过滤</p><p>行处理：使用外连接时将副表的过滤条件写到on(副表的查询)中，否则会先全表关联，在进行过滤。</p><p>列处理:只拿需要的列，少用select *，只读取需要的分区。</p></li><li><p>in &#x2F; exists语句</p><p>在sql中多用exists，在hive中用left semi join 实现会更好。</p></li><li><p>小表、大表join(MapJOIN)</p><p>默认开启；</p><p>MAPJION会把小表全部读入内存中，在map阶段直接拿另外一个表的数据和内存中表数据做匹配，由于在map是进行了join操作，省去了reduce运行的效率也会高很多</p></li><li><p>大表、大表Join</p><p>a.空key过滤：如果join条件中有key为异常数据，则条件过滤；</p><p>b.空key转换：可以表中 key 为空的字段赋一个随机的值</p><p>c.SMB：分桶表join</p></li><li><p>笛卡尔积</p><p>join不加on条件或者无效，会使用1个reduce完成笛卡尔积，设置为严格模式，不允许出现笛卡尔积</p></li><li><p>Count(distinct) </p><p>此操作需要一个reduce task完成，可以用group by + count 替换</p></li><li><p>union(去重) union all (不去重)</p></li></ul><h5 id="job优化"><a href="#job优化" class="headerlink" title="job优化"></a>job优化</h5><ol><li><p>map优化</p><p>MapTask数&#x3D;文件数，尽量减少小文件；</p><p>a. 小文件进行合并：在Map前合并小文件(CombineHiveInputFormat)、在输出合并小文件（merge）；</p><p>b. 复杂文件增加Map数：当input的文件都很大，任务逻辑复杂，可以增加Map数(使每个map处理的数据量减少，减小max可以增加map数，增大max可以减少map数)；</p><p>c. 开启map端聚合combiner（不影响最终业务逻辑）；</p></li><li><p>reduce优化</p><p>一个reduce会生成一个文件，启动和初始化也会消耗时间；</p><p>处理大数据量利用合适的Reduce数；使单个Reduce任务处理数据量大小要合适；</p></li><li><p>整体优化</p><p>a.Fetch抓取：在某些情况不会使用MR计算；</p><p>b.小数据集使用本地模式( set hive.exec.mode.local.auto&#x3D;true;)；</p><p>c.数据量大使用并行执行(set hive.exec.parallel&#x3D;true;)；</p><p>d.严格模式(hive.strict.checks)：分区表使用分区过滤、order by 指定limit、笛卡尔积</p><p>e.小文件过多JVM重用</p><p>f.压缩</p></li></ol><h4 id="数据倾斜-1"><a href="#数据倾斜-1" class="headerlink" title="数据倾斜"></a>数据倾斜</h4><p>少数任务相比下执行很慢。</p><h5 id="单表"><a href="#单表" class="headerlink" title="单表"></a>单表</h5><ol><li>Group by -&gt; 进行负载均衡(set hive.groupby.skewindata &#x3D; true;)、map端进行聚合(hive.map.aggr)</li><li>多个相同key导致 -&gt;增加Reduce数量、自定义分区器、加随机数或过滤</li></ol><h5 id="Join数据倾斜优化"><a href="#Join数据倾斜优化" class="headerlink" title="Join数据倾斜优化"></a>Join数据倾斜优化</h5><ol><li>MapJoin,将小表读入内存；</li><li>使用参数：如果join的键对应的记录数超过(hive.skewjoin.key)则进行分拆</li></ol><h4 id="hive分隔符和空值"><a href="#hive分隔符和空值" class="headerlink" title="hive分隔符和空值"></a>hive分隔符和空值</h4><p>在建表时通过<code>FIELDS TERMINATED BY</code> 指定分割符；</p><p>null实际在HDFS中默认存储为’\N’；可以自定义底层用什么表示null；判断使用a is null 或者 a&#x3D;’\N’；处理使用nvl,case,coalcsce；</p><h4 id="引擎对比"><a href="#引擎对比" class="headerlink" title="引擎对比"></a>引擎对比</h4><p>Mr引擎：多job串联，基于磁盘，落盘的地方比较多;</p><p>Spark引擎：Shuffle过程中落盘,兼顾了可靠性和效率;</p><p>Tez引擎：完全基于内存，将多个有依赖的作业转换为一个作业，只需写一次HDFS。</p><hr><h3 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a>Sqoop</h3><h4 id="sqoop参数"><a href="#sqoop参数" class="headerlink" title="sqoop参数"></a>sqoop参数</h4><pre class="line-numbers language-sqoop" data-language="sqoop"><code class="language-sqoop">&#x2F;opt&#x2F;module&#x2F;sqoop&#x2F;bin&#x2F;sqoop import \--connect \--username \--password \--target-dir \--delete-target-dir \--num-mappers \--fields-terminated-by   \--query   &quot;$2&quot; &#39; and $CONDITIONS;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="导入导出null存储一致性"><a href="#导入导出null存储一致性" class="headerlink" title="导入导出null存储一致性"></a>导入导出null存储一致性</h4><p>hive中null底层表示为’\N’ ，mysql中表示为Null，为保持导入 &#x2F; 出一致：</p><p>导入-&gt;–null-string和–null-non-string</p><p>导出-&gt;–input-null-string和–input-null-non-string</p><h4 id="数据导出一致性"><a href="#数据导出一致性" class="headerlink" title="数据导出一致性"></a>数据导出一致性</h4><p>指导出部分成功数据导致数据出错，使用–staging-table 充当用于暂存导出数据的辅助表以解决</p><h4 id="导出Parquet"><a href="#导出Parquet" class="headerlink" title="导出Parquet"></a>导出Parquet</h4><p>如果是列式存储则不能直接导出。</p><ol><li>创建临时表保存parquet</li><li>将临时表导出</li></ol><hr><h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><hr><h3 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h3><ul><li>变量和数据类型以及隐式转换（var 变量val 常量）</li><li>流程控制（if-else、for、while）以及模式匹配（switch case  样例类）</li><li>函数式编程(高阶函数、匿名函数、函数柯里化、函数参数以及函数至简原则、闭包)</li><li>面向对象(特质-&gt;第一个用extends后面用with；apply)</li><li>集合（List，Array，Map及函数）</li><li>异常及泛型</li></ul><p><img src="C:\Users\86188\AppData\Roaming\Typora\typora-user-images\1651889731844.png" alt="1651889731844"></p><hr><h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><p>解决海量数据的分析计算；先申请资源，在进行计算；</p><h4 id="运行框架"><a href="#运行框架" class="headerlink" title="运行框架"></a>运行框架</h4><p><img src="C:\Users\86188\AppData\Roaming\Typora\typora-user-images\1651829163177.png" alt="1651829163177"></p><ol><li>构建Application环境，申请资源；</li><li>启动Executor,与Cluster Manager维持心跳；</li><li>Spark Context构建DAG，将 DAG 图分解成多个 Stage,任务调度器将task分发；</li><li>Executor执行任务；</li></ol><h5 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h5><ul><li>将用户程序转化为作业（job）</li><li>在 Executor 之间调度任务(task)</li><li>跟踪Executor 的执行情况</li><li>通过UI 展示查询运行情况</li><li>算子以外的代码都是在Driver 端执行</li></ul><h5 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h5><ul><li>运行在工作节点（Worker）中的一个JVM 进程，是整个集群中的专门用于计算的节点；</li><li>RDD 直接缓存 Executor 进程内；</li><li>算子里面的代码都是在Executor 端执行</li></ul><h5 id="Master-amp-Worker"><a href="#Master-amp-Worker" class="headerlink" title="Master &amp; Worker"></a>Master &amp; Worker</h5><p>独立部署环境中</p><ul><li>Master是一个进程，负责资源的调度和分配，并进行集群的监控等；</li><li>Worker 运行在集群中的一台服务器上，由 Master 分配资源对数据进行并行的处理和计算；</li></ul><h5 id="ApplicationMaster"><a href="#ApplicationMaster" class="headerlink" title="ApplicationMaster"></a>ApplicationMaster</h5><p>用于向资源调度器申请Container容器，资源与计算的解耦；</p><h4 id="Spark运行模式"><a href="#Spark运行模式" class="headerlink" title="Spark运行模式"></a>Spark运行模式</h4><ul><li>Local：运行在一台机器上。 测试用。</li><li>Standalone：是Spark自身的一个调度系统。 </li><li>Yarn：采用Hadoop的资源调度器</li></ul><h4 id="Spark常用端口号"><a href="#Spark常用端口号" class="headerlink" title="Spark常用端口号"></a>Spark常用端口号</h4><ul><li>4040:spark-shell 任务端口</li><li>7077：内部通讯端口</li><li>8080：任务执行的端口</li><li>18080：历史服务器</li></ul><h4 id="RDD五大属性"><a href="#RDD五大属性" class="headerlink" title="RDD五大属性"></a>RDD五大属性</h4><p>rdd主要是将逻辑封装，生成task发送到executor执行计算</p><ol><li><p>分区列表</p><p>rdd是被分区的，每个分区都会被一个计算任务(Task)处理，用于执行任务时并行计算；</p></li><li><p>每个分区都有计算函数</p><p>使用分区函数对每个分区计算；</p></li><li><p>基于其他rdd的列表</p><p>rdd之间的依赖关系(血缘)：宽依赖(有shuffle)和窄依赖(每个上游rdd最多被下游的一个分区使用-&gt;独生子女)；</p></li><li><p>key-value数据类型的分区器</p><p>当数据为 KV 时，可以通过(Hash分区 &#x2F; Range分区 &#x2F; 用户自定义分区)指定分区；</p></li><li><p>首选位置</p><p>计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算；</p></li></ol><h4 id="RDD转换算子"><a href="#RDD转换算子" class="headerlink" title="RDD转换算子"></a>RDD转换算子</h4><p>value: map &#x2F; mapPartitions &#x2F; mapPartitionsWithIndex &#x2F; flatMap &#x2F; glom &#x2F; groupBy &#x2F; filter &#x2F; sample &#x2F; distinct &#x2F; coalesce &#x2F; repartition &#x2F; sortBy</p><p>双value：intersection &#x2F; union &#x2F; subtract &#x2F; zip</p><p>key-value: partitionBy &#x2F; reduceByKey &#x2F; groupByKey &#x2F; aggregateByKey &#x2F; foldByKey &#x2F; combineByKey &#x2F; sortByKey &#x2F; join &#x2F; leftOuterJoin &#x2F; cogroup</p><h4 id="RDD行动算子"><a href="#RDD行动算子" class="headerlink" title="RDD行动算子"></a>RDD行动算子</h4><p>reduce &#x2F; collect &#x2F; count &#x2F; first &#x2F; take &#x2F; takeOrdered &#x2F; aggregate &#x2F; fold &#x2F; countByKey &#x2F; foreach &#x2F; save相关</p><p>当spark与数据库操作时，由于数据算子内的操作都是在executor端做的，所以需要有闭包的问题，可以使用foreachPartition代替foreach，在前者中获取数据库连接，那样只需要在一个分区内连接一次了。</p><h4 id="RDD-序列化"><a href="#RDD-序列化" class="headerlink" title="RDD 序列化"></a>RDD 序列化</h4><p>在函数式编程中算子内会用到算子外的数据，形成闭包的效果，由于算子里面的代码都是在Executor端执行的，所以要将数据发送到Executor中，所以需要序列化。</p><p>Kryo 序列化框架：</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala"><span class="token comment">// 替换默认的序列化机制</span><span class="token punctuation">.</span>set<span class="token punctuation">(</span><span class="token string">"spark.serializer"</span><span class="token punctuation">,</span><span class="token string">"org.apache.spark.serializer.KryoSerializer"</span><span class="token punctuation">)</span><span class="token comment">// 注册需要使用 kryo 序列化的自定义类</span><span class="token punctuation">.</span>registerKryoClasses<span class="token punctuation">(</span>Array<span class="token punctuation">(</span>classOf<span class="token punctuation">[</span>Searcher<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>序列化需要继承Serializable 接口</p><h4 id="RDD-持久化"><a href="#RDD-持久化" class="headerlink" title="RDD 持久化"></a>RDD 持久化</h4><ul><li><p>cache:将数据存储在磁盘、内存等，不切断血缘依赖；</p></li><li><p>checkpoint：将数据存储在HDFS等高可用的文件系统，切断血缘关系；</p><pre class="line-numbers language-scala" data-language="scala"><code class="language-scala">rdd<span class="token punctuation">.</span>cache<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">//rdd默MEMORY_ONLY</span>df<span class="token punctuation">.</span>cache<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">//dataframe默MEMORY_AND_DISK</span><span class="token comment">// 设置检查点路径</span>sc<span class="token punctuation">.</span>setCheckpointDir<span class="token punctuation">(</span><span class="token string">"./checkpoint1"</span><span class="token punctuation">)</span>rdd<span class="token punctuation">.</span>checkpoint<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>建议对checkpoint()的RDD 使用Cache 缓存</p></li></ul><h4 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h4><p>将Executor 端变量信息聚合到Driver 端。</p><h5 id="系统累加器"><a href="#系统累加器" class="headerlink" title="系统累加器"></a>系统累加器</h5><ol><li>声明<code>sc.longAccumulator(&quot;name&quot;)</code></li><li>使用<code>acc.add()</code></li><li>取值 <code>acc.value</code></li></ol><h5 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h5><ol><li>继承 AccumulatorV2，并设定泛型</li><li>重写累加器的抽象方法</li></ol><h4 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h4><p>向所有工作节点发送一个较大的只读值。</p><ol><li>声明：<code>sc.broadcast(较大的对象)</code></li><li>使用：<code>broadcast.value</code></li></ol><h4 id="任务的划分"><a href="#任务的划分" class="headerlink" title="任务的划分"></a>任务的划分</h4><ul><li>Application：初始化一个SparkContext即生成一个Application；</li><li>Job：一个Action算子就会生成一个Job；</li><li>Stage：Stage等于宽依赖的个数加1；</li><li>Task：一个Stage阶段中，最后一个RDD的分区个数就是Task的个数。</li></ul><h4 id="RDD、DataFrame、DataSet三者的转换"><a href="#RDD、DataFrame、DataSet三者的转换" class="headerlink" title="RDD、DataFrame、DataSet三者的转换"></a>RDD、DataFrame、DataSet三者的转换</h4><p><img src="C:\Users\86188\AppData\Roaming\Typora\typora-user-images\1651843653930.png" alt="1651843653930"></p><ul><li><p>DataFrame每一行的类型固定为Row，通过解析可以获取各个字段值；</p></li><li><p>DataSet每一行的类型是不一定的，通过getAS方法或者模式匹配拿出特定字段；</p></li></ul><h4 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h4><h5 id="参数调整"><a href="#参数调整" class="headerlink" title="参数调整"></a>参数调整</h5><ul><li>auto.offset.reset -&gt; earliest 从最初始偏移量开始消费数据；</li><li>spark.streaming.kafka.maxRatePerPartition 从kafka分区每秒拉取的条数(延迟)；</li><li>spark.streaming.backpressure.enabled 背压机制，根据延迟动态消费Kafka数据；</li><li>spark.streaming.stopGracefullyOnShutdown 在JVM关闭时正常StreamingContext;</li><li>默认分区数与对接Kafka topic 分区个数一致；</li><li>一个stage耗时由最慢task决定；</li></ul><h5 id="Kafka数据源"><a href="#Kafka数据源" class="headerlink" title="Kafka数据源"></a>Kafka数据源</h5><p>Receiver：需要一个专门executor接收数据然后发到计算executor中，接收数据的Executor和计算的Executor 速度会有所不同，可能会导致内存溢出；</p><p>Direct：使用Kafka的api，Spark Streaming自己就负责追踪消费的offset，并保存在checkpoint中。可以保证数据消费一次且仅消费一次。</p><h5 id="无状态-x2F-有状态转换"><a href="#无状态-x2F-有状态转换" class="headerlink" title="无状态&#x2F;有状态转换"></a>无状态&#x2F;有状态转换</h5><ul><li><p>transform</p><p>将DStream转换为rdd操作；</p></li><li><p>UpdateStateByKey</p><p>需要对检查点目录进行配置，会使用检查点来保存状态；</p></li><li><p>WindowOperations</p><p>在原来SparkStreaming计算批次上封装，增加窗口时长以及滑动步长：必须为采集周期大小的整数倍；</p></li></ul><h4 id="Spark-数据倾斜"><a href="#Spark-数据倾斜" class="headerlink" title="Spark 数据倾斜"></a>Spark 数据倾斜</h4><ul><li><p>表现</p><p>部分任务极慢，任务卡在某个阶段不能结束，任务突然失败</p></li><li><p>原因</p><p>数据激增，Key分布不均，建表连接性差；</p></li><li><p>思路</p><p>数据预处理，异常值过滤；</p><p>数据激增可以单独对这部分数据做两次MR先打散在聚合；</p><p>优化代码逻辑，优化sql(每个行动算子都会增加job,每次shuffle都会增加一个阶段)、调参(Hadoop切片、预聚合、合并，hive中MapJoin等)；</p></li><li><p>定位数据倾斜</p><ol><li>通过web UI或者日志查看task运行情况，定位是哪个stage出现问题；</li><li>代码中在spark或sparkSQL中出现shuffle语言，那么就可以分出前后两个stage，定位出现问题的stage，优化代码逻辑。</li></ol></li><li><p>方法</p><ul><li>hive中ETL处理数据(在hive中预先将key聚合，spark对预处理的hive表操作) -&gt;提升性能但是在hive ETL中还是会倾斜；</li><li>过滤掉少数导致倾斜的key(filter&#x2F;where) -&gt; 适用于倾斜的key少并且对计算本身影响不大的情况;</li><li>提高shuffle操作的并行度(在执行shuffle算子的时候传入参数设置spark.sql.shuffle.partitions，代表了并行度，默认200)-&gt;适用于相同key数据不多情况；</li><li>两阶段聚合（局部聚合+全局聚合）先将key打上随机数前缀后聚合，然后将前缀去掉全局聚合 -&gt; 适用于聚合类的shuffle操作；</li><li>将reduce join转为map join(将较小的rdd直接collect拉到内存，然后对其创建广播变量，遍历较大的rdd与变量合并) -&gt; 适用于大表和小表join的情况；</li><li>将倾斜key打散加前缀join后union合并（采样倾斜key过滤出作为新rdd并加上n以内的随机前缀,将需要join的另一个rdd也过滤出倾斜key膨胀并加上前缀，之后两个rdd进行join；另外两个正常rdd照常join后union合并得到最终join结果）(将另一个rdd膨胀是为了与倾斜key的随机前缀连接上) -&gt; 少量Key倾斜；</li><li>使用随机前缀和扩容RDD进行join（同上但对整个rdd扩容）-&gt; 缓解数据倾斜，对内存资源要求高；</li></ul><p>将多种方案结合：a-&gt;b-&gt;c-&gt;(d&#x2F;e&#x2F;f&#x2F;g)</p></li></ul><hr><p>Flink</p><hr><h3 id="基本算法"><a href="#基本算法" class="headerlink" title="基本算法"></a>基本算法</h3><hr><h3 id="手写HQL"><a href="#手写HQL" class="headerlink" title="手写HQL"></a>手写HQL</h3><h4 id="用户行为路径"><a href="#用户行为路径" class="headerlink" title="用户行为路径"></a>用户行为路径</h4><h5 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h5><p>统计每次跳转的次数使用桑基图展示。</p><ul><li>用户访问路径的可视化通常使用桑基图。如下图所示，该图可真实还原用户的访问路径，包括页面跳转和页面访问次序。</li><li>桑基图需要我们提供每种页面跳转的次数，每个跳转由source&#x2F;target表示，source指跳转起始页面，target表示跳转终到页面。</li><li>注意：<ul><li>source不允许为空，但target可为空</li><li>桑基图所展示的流程不允许存在环</li></ul></li></ul><h5 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h5><p>对source，target分组即可，为了不存在环，将每个用户的访问加前缀以确定步骤。</p><h5 id="语句实现"><a href="#语句实现" class="headerlink" title="语句实现"></a>语句实现</h5><h4 id="用户变动统计"><a href="#用户变动统计" class="headerlink" title="用户变动统计"></a>用户变动统计</h4><h5 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h5><p>统计截至本日流失用户和回流用户的人数</p><ul><li>流失用户：末次活跃时间为7日前的用户</li><li>回流用户：末次活跃时间为今日，上次活跃时间在8日前的用户</li></ul><h5 id="思路分析-1"><a href="#思路分析-1" class="headerlink" title="思路分析"></a>思路分析</h5><p>流失用户数量为截至今日用户最后登录时间为七天前；</p><p>回流用户数量为截至今日为用户最后登录时间且昨天最后登录时间为八天前；</p><h5 id="语句实现-1"><a href="#语句实现-1" class="headerlink" title="语句实现"></a>语句实现</h5><h4 id="连续问题"><a href="#连续问题" class="headerlink" title="连续问题"></a>连续问题</h4><h5 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a>问题描述</h5><p>统计连续3天及以上减少碳排放量在100以上的用户</p><h5 id="思路分析-2"><a href="#思路分析-2" class="headerlink" title="思路分析"></a>思路分析</h5><ol><li>按照用户和时间分组求每个用户每天减少碳排量的数量并过滤排放量在100以上；</li><li>按照时间求每个用户的rank；</li><li>将时间减去排名如果相同则时间连续(等差数列)；</li><li>按照用户和时间(等差列)分组求数量大于等于3的记录；</li></ol><h5 id="语句实现-2"><a href="#语句实现-2" class="headerlink" title="语句实现"></a>语句实现</h5><h4 id="分组问题"><a href="#分组问题" class="headerlink" title="分组问题"></a>分组问题</h4><h5 id="问题描述-3"><a href="#问题描述-3" class="headerlink" title="问题描述"></a>问题描述</h5><p>用户连续访问记录时间间隔如果小于60秒，则分为同一组，输出用户的组号。</p><h5 id="思路分析-3"><a href="#思路分析-3" class="headerlink" title="思路分析"></a>思路分析</h5><ol><li>计算上次访问时间与本次访问时间差如果小于60秒，则计数为0，否则为1；</li><li>求和开窗计算用户的计数；</li></ol><h5 id="语句实现-3"><a href="#语句实现-3" class="headerlink" title="语句实现"></a>语句实现</h5><h4 id="间隔连续问题"><a href="#间隔连续问题" class="headerlink" title="间隔连续问题"></a>间隔连续问题</h4><h5 id="问题描述-4"><a href="#问题描述-4" class="headerlink" title="问题描述"></a>问题描述</h5><p>计算每个用户最大的连续登录天数，可以间隔一天。解释：如果一个用户在1,3,5,6登录游戏，则视为连续6天登录。</p><h5 id="思路分析-4"><a href="#思路分析-4" class="headerlink" title="思路分析"></a>思路分析</h5><ol><li>计算上次登录时间与本次登录时间相差天数diff_time；</li><li>如果diff_time&lt;2则为连续登录(分为同一组)，否则重新计数(登录时间超过两天)；</li><li>统计用户在同一组的连续登陆天数(最大时间-最小时间+1)；</li><li>取连续登录天数的最大值；</li></ol><h5 id="语句实现-4"><a href="#语句实现-4" class="headerlink" title="语句实现"></a>语句实现</h5><h4 id="打折日期交叉问题"><a href="#打折日期交叉问题" class="headerlink" title="打折日期交叉问题"></a>打折日期交叉问题</h4><h5 id="问题描述-5"><a href="#问题描述-5" class="headerlink" title="问题描述"></a>问题描述</h5><p> 计算每个品牌总的打折销售天数，注意其中的交叉日期，比如vivo品牌，第一次活动时间为2021-06-05到2021-06-15，第二次活动时间为2021-06-09到2021-06-21其中9号到15号为重复天数，只统计一次，即vivo总打折天数为2021-06-05到2021-06-21共计17天。</p><h5 id="思路分析-5"><a href="#思路分析-5" class="headerlink" title="思路分析"></a>思路分析</h5><ol><li>由于有时间重复，如果某品牌上一个活动的结束日期比下一个活动开始时间大（时间重复），则将下一个活动的开始时间设置为上一个活动结束日期加一（将两个活动时间连接起来）否则没有时间重复问题；</li><li>计算每次活动天数：结束日期-开始日期+1(如果为负数则说明上一个活动时间包括了本次时间，不用加入统计)；</li><li>按照品牌求和活动天数；</li></ol><h5 id="语句实现-5"><a href="#语句实现-5" class="headerlink" title="语句实现"></a>语句实现</h5><h4 id="同时在线问题"><a href="#同时在线问题" class="headerlink" title="同时在线问题"></a>同时在线问题</h4><h5 id="问题描述-6"><a href="#问题描述-6" class="headerlink" title="问题描述"></a>问题描述</h5><p>计算出平台最高峰同时在线的主播人数。</p><h5 id="思路分析-6"><a href="#思路分析-6" class="headerlink" title="思路分析"></a>思路分析</h5><ol><li>可以将开始时间认为人数+1，结束时间人数-1；</li><li>按照时间排序，求截至当前时间的人数cnt_s；</li><li>求最大的cnt_s即为所求；</li></ol><h5 id="语句实现-6"><a href="#语句实现-6" class="headerlink" title="语句实现"></a>语句实现</h5><hr><h3 id="Mysql"><a href="#Mysql" class="headerlink" title="Mysql"></a>Mysql</h3><h4 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h4><p>索引是一个单独的、存储在磁盘上的数据库结构，包含对数据表里所有记录的引用指针。使用索引可以快速找到在某个或多个列中有一特定值的行。</p><h4 id="索引失效常见情况"><a href="#索引失效常见情况" class="headerlink" title="索引失效常见情况"></a>索引失效常见情况</h4><ul><li>没有符合最左前缀原则；1、2、3</li><li>字段进行了隐私数据类型转换；4、5、6</li><li>走索引没有全表扫描快；7</li></ul><ol><li>sql语句中有or关键字；</li><li>复合索引未用左列字段;</li><li>like以%开头;</li><li>需要类型转换;</li><li>where中索引列有运算;</li><li>where中索引列使用了函数;</li><li>如果mysql觉得全表扫描更快时（数据少）;</li></ol><h4 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h4><ul><li><h5 id="原子性"><a href="#原子性" class="headerlink" title="原子性"></a>原子性</h5></li><li><h5 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a>一致性</h5></li><li><h5 id="隔离性"><a href="#隔离性" class="headerlink" title="隔离性"></a>隔离性</h5></li><li><h5 id="持久性"><a href="#持久性" class="headerlink" title="持久性"></a>持久性</h5></li></ul><h4 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h4><h4 id="Binlog解析工具"><a href="#Binlog解析工具" class="headerlink" title="Binlog解析工具"></a>Binlog解析工具</h4><p>Maxwell是一个能实时读取<a href="https://cloud.tencent.com/product/cdb?from=10680">MySQL</a>二进制日志binlog，并生成 JSON 格式的消息，作为生产者发送给 Kafka，Kinesis、RabbitMQ、Redis、Google Cloud Pub&#x2F;Sub、文件或其它平台的应用程序。</p><hr><h3 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h3><p>面向<strong>分析</strong>的存储系统。</p><h4 id="与数据库区别："><a href="#与数据库区别：" class="headerlink" title="与数据库区别："></a>与数据库区别：</h4><p>数据库是面向事务的设计，数仓是面向<strong>主题</strong>的设计。</p><ul><li>存储：数据库存储在线交易数据，数仓存储一般都是历史数据；</li><li>设计：数据库一般采用符合范式的规则来设计，数仓为了便于查询加入了宽表，虽然增加了冗余但提高了速度；</li><li>目的：数据库为了捕获数据而设计，数仓为了分析数据而设计，他的两个基本元素是维度表和事实表。</li></ul><h4 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h4><h5 id="关系建模"><a href="#关系建模" class="headerlink" title="关系建模"></a>关系建模</h5><p>将数据抽象为两个概念–实体和关系，并用规范化的方式表示出来，关系模型遵守第三范式(表中不允许存在非主属性的传递函数依赖)，数据一致性得到保证。</p><h5 id="维度建模"><a href="#维度建模" class="headerlink" title="维度建模"></a>维度建模</h5><p>以数据分析作为出发点，不遵循第三范式，故存在一定冗余，维度模型面向业务，将业务用事实表和维度表呈现出来。</p><p><strong>维度表</strong>对事实的描述信息，对应世界中的一个对象或者概念，比如日期、地区、用户等；</p><p><strong>事实表</strong>每行数据代表一个业务事件，比如下单、支付、购物车等；</p><h4 id="数仓分层"><a href="#数仓分层" class="headerlink" title="数仓分层"></a>数仓分层</h4><h5 id="好处"><a href="#好处" class="headerlink" title="好处"></a>好处</h5><ul><li>清晰数据结构：让每个数据层都有自己的作用和职责，在使用和维护时能更快的定位和理解；</li><li>复杂问题简化：将复杂问题拆解成多个步骤完成，每层只解决单一的问题；</li><li>统一数据口径：通过数据分层提供统一的数据出口，统一输出口径；</li><li>减少重复开发：规范数据分层，开发一些通用的中间层，减少重复计算；</li></ul><h5 id="原始数据层ODS"><a href="#原始数据层ODS" class="headerlink" title="原始数据层ODS"></a>原始数据层ODS</h5><p>数据源的数据经过抽取、洗净、传输(ETL)进入本层。</p><ul><li>保持数据原貌不做修改，起到备份数据的作用；</li><li>数据采用压缩，减少磁盘存储；</li><li>创建分区表，防止全表扫描；</li></ul><h5 id="明细数据层DWD"><a href="#明细数据层DWD" class="headerlink" title="明细数据层DWD"></a>明细数据层DWD</h5><ul><li>主要对ODS层的数据做一些清洗和规范化的操作，比如说去除空值、脏数据，脱敏等；</li><li>为了提高数据明细层的易用性，该层通常会采用一些维度退化的方法；</li><li>一般dwd层采用最小粒度以便后续统计细粒度的指标；</li><li>一行数据代表一次业务行为；</li></ul><h5 id="服务数据层DWS"><a href="#服务数据层DWS" class="headerlink" title="服务数据层DWS"></a>服务数据层DWS</h5><p>在DWD层对数据做一些轻微的聚合操作，生成一些列的中间结果表，提升公共指标的复用性，减少重复加工的工作，主要存放主题对象当天的汇总行为。</p><h5 id="数据主题层DWT"><a href="#数据主题层DWT" class="headerlink" title="数据主题层DWT"></a>数据主题层DWT</h5><p>在DWM的基础上，整合汇总成分析某个主题域的宽表，存放所有主题的累计行为。</p><h5 id="DIM层"><a href="#DIM层" class="headerlink" title="DIM层"></a>DIM层</h5><ol><li>选择业务过程；</li><li>声明粒度；</li><li>确定维度；</li><li>确定事实；</li></ol><h5 id="数据应用层ADS"><a href="#数据应用层ADS" class="headerlink" title="数据应用层ADS"></a>数据应用层ADS</h5><p>对主题进行指标分析，提供数据产品和数据分析使用的数据，做可视化展示；</p><h4 id="处理过的业务"><a href="#处理过的业务" class="headerlink" title="处理过的业务"></a>处理过的业务</h4><p>大量sql写到Kafka怎么办</p><p>结构化思维：确定目标，资源分析，制定计划。</p><p>薪资是能力的体现</p><p><img src="C:\Users\86188\AppData\Roaming\Typora\typora-user-images\1654249976959.png" alt="1654249976959"></p>]]></content>
      
      
      <categories>
          
          <category> big-data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> big-data </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kettle使用(1)-更换界面logo和文字</title>
      <link href="/2022/07/10/kettle-1-geng-huan-tu-biao-wen-zi/"/>
      <url>/2022/07/10/kettle-1-geng-huan-tu-biao-wen-zi/</url>
      
        <content type="html"><![CDATA[<h1 id="kettle使用-1-更换界面logo和文字"><a href="#kettle使用-1-更换界面logo和文字" class="headerlink" title="kettle使用(1)-更换界面logo和文字"></a>kettle使用(1)-更换界面logo和文字</h1><p>kettle主要用于ETL，因为操作简单、门槛低被很多企业使用，今天来说一下不修改源码如何更改kettle的logo和其中的文字(包括标题)。</p><p>本文中 <code>kettle安装目录</code> 指代为 <code>%kettle%</code> 。</p><p><strong>在 <code>%kettle%\data-integration\lib\</code> 中找到 <code>kettle-ui-swt-9.3.0.0-428.jar</code> ，使用压缩软件查看。</strong></p><h2 id="修改图标"><a href="#修改图标" class="headerlink" title="修改图标"></a>修改图标</h2><p>替换：自己的图标名称与替换的<strong>名称和扩展名</strong>相同。</p><ul><li>更改logo图标：在<code>ui\images\</code> 找到 <code>spoon.png</code>和 <code>spoon.ico</code> ，将自己的图标按照源文件的像素大小替换(为了更加契合)即可。</li><li>更改tab的图标：在<code>ui\images\</code> 找到 <code>kettle_logo_small.png</code> 和 <code>kettle_logo_small.svg</code> ，注意 <code>svg</code> 格式的文件需要进行格式转换才能得到，在网页中搜索 <code>svg格式转换</code> 转换后替换文件即可。</li><li>启动界面图片：<code>\ui\images\kettle_splash.png</code></li><li>启动界面版本信息：<code>\ui\org\pentaho\di\ui\core\dialog\messages\messages_en_US.properties</code>中的<code>SplashDialog.Version</code><br>启动界面license：<code>\ui\org\pentaho\di\ui\core\dialog\license\license.txt</code></li></ul><p>理论上来说，你在这里找到的图标都可以进行修改替换成自己想要的图片。</p><h2 id="修改文字"><a href="#修改文字" class="headerlink" title="修改文字"></a>修改文字</h2><p>kettle界面中的文字在文件中使用<code>字节码</code>保存的，所以我们在其中搜索和修改都是用<code>字节码</code>格式来进行的，在网页搜索<code>字节码在线转换</code>即可。</p><p>编辑 <code>org\pentaho\di\ui\spoon\messages</code> 下<code>messages_zh_CN.properties</code> ，搜索你要替换的文字，将文字替换即可。标题搜索<code>Spoon.Application.Name</code> 替换内容即可。</p><p>举个例子，现在我将 <code>右键转换弹出的新建</code> 两字替换为 <code>再来一个嘛</code> ：</p><p><img src="/../pic/kettle-1%E6%9B%B4%E6%8D%A2%E5%9B%BE%E6%A0%87%E6%96%87%E5%AD%97/1657431509657.png" alt="1657431509657"></p><ol><li><p>将 <code>新建</code> 转换为<code>ASCII</code>格式 <code>\u65b0\u5efa</code>:</p><p><img src="/../pic/kettle-1%E6%9B%B4%E6%8D%A2%E5%9B%BE%E6%A0%87%E6%96%87%E5%AD%97/1657431563581.png" alt="1657431563581"></p></li><li><p>在<code>messages_zh_CN.properties</code> 中搜索转换后的文字：</p><p><img src="/../pic/kettle-1%E6%9B%B4%E6%8D%A2%E5%9B%BE%E6%A0%87%E6%96%87%E5%AD%97/1657431582300.png" alt="1657431582300"></p><p>注意可能搜索结果不唯一，选择查找结果只有你搜索的内容。</p></li><li><p>将 <code>再来一个嘛</code> 转换为ASCII 格式 <code>\u518d\u6765\u4e00\u4e2a\u561b</code> 后替换：</p><p><img src="/../pic/kettle-1%E6%9B%B4%E6%8D%A2%E5%9B%BE%E6%A0%87%E6%96%87%E5%AD%97/1657431601344.png" alt="1657431601344"></p><p><img src="/../pic/kettle-1%E6%9B%B4%E6%8D%A2%E5%9B%BE%E6%A0%87%E6%96%87%E5%AD%97/1657431620281.png" alt="1657431620281"></p></li><li><p>保存退出查看效果：</p><p><img src="/../pic/kettle-1%E6%9B%B4%E6%8D%A2%E5%9B%BE%E6%A0%87%E6%96%87%E5%AD%97/1657431643972.png" alt="1657431643972"></p></li></ol><p>理论上来说，你在这里找到的文字都可以进行修改替换成自己想要的文字；理论上来说，文字相关的文件都在messages文件夹下。</p><p>更多示例：<a href="http://www.kettle.org.cn/category/demo">kettle示例 – Kettle中文网</a></p>]]></content>
      
      
      <categories>
          
          <category> kettle </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kettle </tag>
            
            <tag> ETL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kettle使用(2)-使用sql脚本和Java脚本</title>
      <link href="/2022/07/10/kettle-2-shi-yong-sql-he-java-jiao-ben/"/>
      <url>/2022/07/10/kettle-2-shi-yong-sql-he-java-jiao-ben/</url>
      
        <content type="html"><![CDATA[<h1 id="kettle使用-2-使用SQL和JAVA脚本"><a href="#kettle使用-2-使用SQL和JAVA脚本" class="headerlink" title="kettle使用(2)-使用SQL和JAVA脚本"></a>kettle使用(2)-使用SQL和JAVA脚本</h1><h2 id="kettle中执行SQL脚本"><a href="#kettle中执行SQL脚本" class="headerlink" title="kettle中执行SQL脚本"></a>kettle中执行SQL脚本</h2><ul><li>在转换中需要勾选 <em><strong>执行每一行* *变量替换</strong></em>；</li><li>SQL中如果字段是字符类型的需要加引号**’?’**;</li><li>SQL中的变量用**?<strong>&#x2F;</strong>${var}**(测试有问题)代替;</li><li><strong>?<strong>为变量时：在</strong>作为参数的字段</strong>中顺序填入变量(需要一一对应)，定义参数需要将文件作为输入，里面的字段值就是参数；</li><li>${var}为变量时：在<strong>转换属性</strong>的<strong>命名参数</strong>中设置变量；</li></ul><h2 id="kettle中执行Java脚本"><a href="#kettle中执行Java脚本" class="headerlink" title="kettle中执行Java脚本"></a>kettle中执行Java脚本</h2><p>添加变量：</p><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token class-name">Type</span> <span class="token keyword">var</span> <span class="token operator">=</span> <span class="token string">"randtext"</span><span class="token punctuation">;</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token class-name">Fields<span class="token punctuation">.</span>Out</span><span class="token punctuation">,</span><span class="token string">"var"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setValue</span><span class="token punctuation">(</span>r<span class="token punctuation">,</span><span class="token keyword">var</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">//并在底部'字段'中声明变量var对应类型Type</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>获取输入的字段：</p><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token class-name">Type</span> <span class="token keyword">var</span> <span class="token operator">=</span> <span class="token function">get</span><span class="token punctuation">(</span><span class="token class-name">Fields<span class="token punctuation">.</span>In</span><span class="token punctuation">,</span> <span class="token string">"a_fieldname"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span>r<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">//表中字段名为a_fieldname的字段</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>输出字段：</p><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token function">get</span><span class="token punctuation">(</span><span class="token class-name">Fields<span class="token punctuation">.</span>Out</span><span class="token punctuation">,</span> <span class="token string">"output_fieldname"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setValue</span><span class="token punctuation">(</span>r<span class="token punctuation">,</span> <span class="token keyword">var</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="kettle中设置定时"><a href="#kettle中设置定时" class="headerlink" title="kettle中设置定时"></a>kettle中设置定时</h2><p>在作业的<strong>start</strong>组件设置</p><h2 id="连接池"><a href="#连接池" class="headerlink" title="连接池"></a>连接池</h2><p>会在目标数据库生成对应表文件，右上角connect连接就可以了。</p><p>报错：</p><pre class="line-numbers language-none"><code class="language-none">You don&#39;t seem to be getting a connection to the server. Please check the path you&#39;re using and make sure the server is up and running. <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>解决方法：1.是否缺失jar包；2.相关服务是否开启；3.如果没有对应的R_的表，查看对应用户在资源库是否又建表权限，正确赋权后，删掉原来的数据库连接，重新配置repository即可；4.新建数据库重新配置</p>]]></content>
      
      
      <categories>
          
          <category> kettle </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kettle </tag>
            
            <tag> ETL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/07/08/hello-world/"/>
      <url>/2022/07/08/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Oracle expdp/impdp总结</title>
      <link href="/2022/07/08/oracle-expdp-impdp-zong-jie/"/>
      <url>/2022/07/08/oracle-expdp-impdp-zong-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="Oracle-expdp-x2F-impdp总结"><a href="#Oracle-expdp-x2F-impdp总结" class="headerlink" title="Oracle expdp&#x2F;impdp总结"></a>Oracle expdp&#x2F;impdp总结</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>数据泵（expdp，impdp）是Oracle 10g时引入的新技术，兼容了之前的数据导出导入工具（exp，imp）大部分功能，并进一步完善，提供了很多新功能以满足复杂的业务需求。区别于传统的exp，imp工具，数据泵相关命令需在数据库<strong>服务端</strong>执行。</p><p>数据泵属于逻辑迁移，可跨操作系统版本，跨数据库版本。高版本兼容低版本，高版本向低版本导数据，导出时需添加低版本的版本号。</p><h2 id="快速开始"><a href="#快速开始" class="headerlink" title="快速开始"></a>快速开始</h2><p>本篇适用于常用的导出导入命令，如果你只想将本库的数据导出到文件然后将文件导入到库，本篇可以作为参考。</p><p>本例将密码为<strong>test</strong>的用户<strong>C##TEST</strong>中的<strong>testcar</strong>表按照<strong>条件</strong>导出到指定目录，并导入后<strong>更名</strong>为<strong>newtestcar</strong>。</p><ol><li><p>在操作系统创建文件目录用来保存导出的dump文件(Windows系统直接图形化建立路径即可)；</p><pre class="line-numbers language-plsql" data-language="plsql"><code class="language-plsql">mkdir <span class="token operator">-</span>p d<span class="token operator">:</span>\test\dump<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>将目录指定为数据库的目录路径对象；</p><pre class="line-numbers language-plsql" data-language="plsql"><code class="language-plsql"><span class="token keyword">create</span> <span class="token keyword">directory</span> dptest02 <span class="token keyword">as</span> <span class="token string">'d:\test\dump'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>为用户赋予读写目录对象的权限；</p><pre class="line-numbers language-plsql" data-language="plsql"><code class="language-plsql"><span class="token keyword">grant</span> <span class="token keyword">read</span><span class="token punctuation">,</span><span class="token keyword">write</span> <span class="token keyword">on</span> <span class="token keyword">directory</span> dptest02 <span class="token keyword">to</span> <span class="token keyword">C</span>##TEST<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>导出；</p><pre class="line-numbers language-basic" data-language="basic"><code class="language-basic">expdp C##TEST<span class="token operator">/</span>test@orcl directory<span class="token operator">=</span>dptest02 tables<span class="token operator">=</span>testcar query<span class="token operator">=</span>\"where car_buy_time between to_date<span class="token punctuation">(</span>'<span class="token number">2018</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">01</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">00</span><span class="token punctuation">:</span><span class="token number">00</span>'<span class="token punctuation">,</span>'yyyy<span class="token operator">-</span>mm<span class="token operator">-</span>dd HH24<span class="token punctuation">:</span>mi<span class="token punctuation">:</span>ss'<span class="token punctuation">)</span> <span class="token operator">and</span> to_date<span class="token punctuation">(</span>'<span class="token number">2019</span><span class="token operator">-</span><span class="token number">12</span><span class="token operator">-</span><span class="token number">31</span> <span class="token number">23</span><span class="token punctuation">:</span><span class="token number">59</span><span class="token punctuation">:</span><span class="token number">59</span>'<span class="token punctuation">,</span>'yyyy<span class="token operator">-</span>mm<span class="token operator">-</span>dd HH24<span class="token punctuation">:</span>mi<span class="token punctuation">:</span>ss'<span class="token punctuation">)</span>\" dumpfile<span class="token operator">=</span>testcar%U.dmp parallel<span class="token operator">=</span><span class="token number">10</span> job_name<span class="token operator">=</span>jobtest01<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li><p>导入；</p><pre class="line-numbers language-basic" data-language="basic"><code class="language-basic">impdp C##TEST<span class="token operator">/</span>test directory<span class="token operator">=</span>dp_test01 dumpfile<span class="token operator">=</span>testcar%U.dmp remap_table<span class="token operator">=</span>C##TEST.testcar<span class="token punctuation">:</span>newtestcar<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ol><p>总文件113G ,条件导出expdp按照条件导出49G用时25分，impdp导入用时26分，并行10个进程，共231938000行；</p><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="1、目标新库上的操作"><a href="#1、目标新库上的操作" class="headerlink" title="1、目标新库上的操作"></a>1、目标新库上的操作</h3><p>（1）创建临时表空间</p><pre class="line-numbers language-plsql" data-language="plsql"><code class="language-plsql"><span class="token keyword">create</span> temporary tablespace 用户临时表空间名称 tempfile <span class="token string">'/u01/tablespaces/user_temp.dbf'</span> <span class="token keyword">size</span> <span class="token number">50</span>m autoextend <span class="token keyword">on</span> next <span class="token number">50</span>m maxsize <span class="token number">20480</span>m extent management <span class="token keyword">local</span><span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>备注：根据实际情况调整表空间大小等参数以及规划临时表空间、回滚表空间等相关规划。</p><p>（2）创建数据表空间</p><pre class="line-numbers language-plsql" data-language="plsql"><code class="language-plsql"><span class="token keyword">create</span> tablespace 用户表空间名称 datafile <span class="token string">'/u01/tablespaces/user_data.dbf'</span> <span class="token keyword">size</span> <span class="token number">50</span>m autoextend <span class="token keyword">on</span> next <span class="token number">50</span>m maxsize <span class="token number">20480</span>m extent management <span class="token keyword">local</span><span class="token punctuation">;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>（3）建立用户，并指定默认表空间</p><pre class="line-numbers language-none"><code class="language-none">create user 用户名称 identified by 密码 default tablespace 用户表空间名称 temporary tablespace 用户临时表空间名称;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>（4）给用户授予权限</p><pre class="line-numbers language-none"><code class="language-none">grant connect,resource to 用户名1;grant create database link to 用户名;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>注意：赋权给多个用户的情况下，各个用户名称间用,分隔即可。</p><p>（5）登录需要创建dblink的用户，创建dblink</p><pre class="line-numbers language-plsql" data-language="plsql"><code class="language-plsql"><span class="token keyword">CREATE</span> DATABASE LINK DBLink名称 <span class="token keyword">CONNECT</span> <span class="token keyword">TO</span> 用户 <span class="token keyword">IDENTIFIED</span> <span class="token keyword">BY</span> 密码 <span class="token keyword">USING</span> <span class="token string">'(DESCRIPTION =(ADDRESS_LIST =(ADDRESS =(PROTOCOL = TCP)(HOST = XXX.XXX.XXX.XXX)(PORT = 1521)))(CONNECT_DATA =(SERVICE_NAME = 实例名)))'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>注意：创建DBLINK默认是用户级别的，对当前用户有效。只有当需要对所有用户有效时，再建立公有的DBlink对象（pulic参数）。</p><h3 id="2、创建数据备份目录（源库和目标库）"><a href="#2、创建数据备份目录（源库和目标库）" class="headerlink" title="2、创建数据备份目录（源库和目标库）"></a>2、创建数据备份目录（源库和目标库）</h3><p>备份目录需要使用操作系统用户创建一个真实的目录，然后登录oracle dba用户，创建逻辑目录，指向该路径并使用户有对其读写权限。这样oracle才能识别这个备份目录。</p><p>（1）在操作系统上建立真实的目录</p><pre class="line-numbers language-none"><code class="language-none">$ mkdir -p d:\test\dump<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（2）登录oracle管理员用户</p><pre class="line-numbers language-qlsql" data-language="qlsql"><code class="language-qlsql">SQL&gt; conn &#x2F;as sysdba<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>（3）创建逻辑目录</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">SQL</span><span class="token operator">></span> <span class="token keyword">create</span> directory dp_name <span class="token keyword">as</span> <span class="token string">'d:\test\dump'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>查看目录是否已经创建成功：</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">SQL</span><span class="token operator">></span> <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> dba_directories<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>5、用sys管理员给指定用户赋予在该目录的操作权限</p><pre class="line-numbers language-plsql" data-language="plsql"><code class="language-plsql"><span class="token keyword">SQL</span><span class="token operator">></span> <span class="token keyword">grant</span> <span class="token keyword">read</span><span class="token punctuation">,</span><span class="token keyword">write</span> <span class="token keyword">on</span> <span class="token keyword">directory</span> dp_name <span class="token keyword">to</span> 用户<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="常用语句"><a href="#常用语句" class="headerlink" title="常用语句"></a>常用语句</h2><ul><li>expdp导出</li></ul><pre class="line-numbers language-none"><code class="language-none">1)导出表expdp  tables&#x3D;dbmon.lihaibo_exp dumpfile&#x3D;sms.dmp DIRECTORY&#x3D;dump_dir;2)并发导出parallel，指定job名我们需要特别注意一点，parallel 一定要与 dumpfile&#x3D;...%U.dmp结合 使用，或者有多个表需要同时导出。单表，或者其他诸如network_link方式，指定parallel,也无法开启并发进程expdp scott&#x2F;tiger@orcl directory&#x3D;dpdata1 dumpfile&#x3D;scott3%U.dmp parallel&#x3D;4 job_name&#x3D;scott33)全表expdp scott&#x2F;tiger@orcl TABLES&#x3D;emp,dept dumpfile&#x3D;expdp.dmp DIRECTORY&#x3D;dpdata1;4)导出表，并指定表中的内容expdp scott&#x2F;tiger@orcl directory&#x3D;dpdata1 dumpfile&#x3D;expdp.dmp Tables&#x3D;emp query&#x3D;&quot;WHERE deptno&#x3D;20&quot;;5)导出表空间expdp system&#x2F;manager DIRECTORY&#x3D;dpdata1 DUMPFILE&#x3D;tablespace.dmp TABLESPACES&#x3D;temp,example;6)导出全库expdp system&#x2F;manager DIRECTORY&#x3D;dpdata1 DUMPFILE&#x3D;full.dmp FULL&#x3D;y;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>impdp导入</li></ul><pre class="line-numbers language-none"><code class="language-none">1) 全用户导入impdp scott&#x2F;tiger DIRECTORY&#x3D;dpdata1 DUMPFILE&#x3D;expdp.dmp SCHEMAS&#x3D;scott;2) 用户对象迁移impdp system&#x2F;manager DIRECTORY&#x3D;dump_dir DUMPFILE&#x3D;expdp.dmp TABLES&#x3D;scott.dept REMAP_SCHEMA&#x3D;scott:system; （SCOTT为原用户，system为目标用户）3) 导入指定表空间impdp system&#x2F;manager DIRECTORY&#x3D;dump_dir DUMPFILE&#x3D;tablespace.dmp TABLESPACES&#x3D;example;4) 全库导入impdb system&#x2F;manager DIRECTORY&#x3D;dump_dir DUMPFILE&#x3D;full.dmp FULL&#x3D;y;5) 表已存在的处理impdp system&#x2F;manager DIRECTORY&#x3D;dump_dir DUMPFILE&#x3D;expdp.dmp SCHEMAS&#x3D;system TABLE_EXISTS_ACTION&#x3D;append;6) 表空间迁移impdp system&#x2F;manager directory&#x3D;dump_dir dumpfile&#x3D;remap_tablespace.dmp logfile&#x3D;remap_tablespace.log remap_tablespace&#x3D;A:B (A为原表空间名，B为指定的目标表空间名)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="expdp-x2F-impdp参数说明"><a href="#expdp-x2F-impdp参数说明" class="headerlink" title="expdp&#x2F;impdp参数说明"></a>expdp&#x2F;impdp参数说明</h2><ul><li><p>帮助：</p><blockquote><pre class="line-numbers language-none"><code class="language-none">impdp -helpexpdp help&#x3D;y<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></blockquote></li><li><p>ATTACH</p></li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    当我们使用ctrl+C 退出交互式命令时，可使用attach参数重新进入到交互模式语法    ATTACH&#x3D;[schema_name.]job_name    Schema_name用户名,job_name任务名示例    Expdp scott&#x2F;tiger ATTACH&#x3D;scott.export_job<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li>CONTENT</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    限制了导出的内容，包括三个级别：全部／数据／元数据（结构）语法   CONTENT&#x3D;&#123;ALL | DATA_ONLY | METADATA_ONLY&#125;   ALL           -- 导出所有数据，包括元数据及数据   DATA_ONLY     -- 只导出数据   METADATA_ONLY -- 只包含元数据，就是创建语句示例   Expdp scott&#x2F;tiger DIRECTORY&#x3D;dump DUMPFILE&#x3D;a.dump CONTENT&#x3D;METADATA_ONLY<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li><strong>DIRECTORY</strong></li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用此路径可以理解为实际绝对路径在oracle数据库里的别名，是导出文件的存储位置    路径的创建： create directory &amp;DIRECTORY_NAME AS &#39;&amp;PATH&#39;;    查看已存在路径： select  * from dba_directories;语法    directory&#x3D;[directory_name]示例    Expdp scott&#x2F;tiger DIRECTORY&#x3D;dump_dir DUMPFILE&#x3D;lhb.dump<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li><strong>DUMPFILE</strong></li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    此参数用户命名导出文件，默认是 expdat.dmp. 文件的存储位置如果在文件名前没有指定directory,则会默认存储到directory参数指定的路径下。语法    DUMPFILE&#x3D;[dump_dir:]file_name示例    Expdp scott&#x2F;tiger DIRECTORY&#x3D;dump_dir DUMPFILE&#x3D;dump_dir1:a.dmp<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li>ESTIMATE</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">在使用Expdp进行导出时，Expdp需要计算导出数据大小容量，Oracle可以通过两种方式进行容量估算，一种是通过数据块(blocks)数量、一种是通过统计信息中记录的内容(statistics)估算.语法结构：    EXTIMATE&#x3D;&#123;BLOCKS | STATISTICS&#125;示例：    Expdp scott&#x2F;tiger TABLES&#x3D;emp ESTIMATE&#x3D;STATISTICS DIRECTORY&#x3D;dump_dir DUMPFILE&#x3D;halberd.dump    Expdp scott&#x2F;tiger TABLES&#x3D;emp ESTIMATE&#x3D;BLOCKS DIRECTORY&#x3D;dump_dir DUMPFILE&#x3D;halberd.dump<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li>EXTIMATE_ONLY</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    此参数用于统计导出的数据量大小及统计过程耗时长短。语法    EXTIMATE_ONLY&#x3D;&#123;Y | N&#125;示例    Expdp scott&#x2F;tiger ESTIMATE_ONLY&#x3D;y NOLOGFILE&#x3D;y directory&#x3D;dump_dir schemas&#x3D;halberd<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li>EXCLUDE</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    此参数用于排除不需要导出的内容，如我们进行全库导出，但是不需要导出用户scott，此时需要在exlude后先指定排除类型为schema,再指定具体的schema。具体使用方法见include参数. EXCLUDE与include的使用方法是一样的语法    EXCLUDE&#x3D;object_type[:name_clause] [,object_type[:name_clause] ]    name_clause        &quot;&#x3D;&#39;object_name&#39;&quot;        &quot;in (&#39;object_name&#39;[,&#39;object_name&#39;,....])&quot;        &quot;in (select_clause) &quot;    Object_type对象类型，如：table,view,procedure,schema等    name_clause指定名称的语句,如果不具体指定是哪个对象，则此类所有对象都不导出, select 语句中表名不要加用户名。用户名，通过schemas 指定。示例    expdp scott&#x2F;tiger DIRECTORY&#x3D;dump_dir DUMPFILE&#x3D;halberd.dup EXCLUDE&#x3D;VIEW    expdp scott&#x2F;tiger DIRECTORY&#x3D;dump_dir DUMPFILE&#x3D;halberd.dup EXCLUDE&#x3D;TABLE:\&quot; IN\(\&#39;TEMP\&#39;,\&#39;GRADE\&#39;\)\&quot;    EXCLUDE&#x3D;TABLE:&quot;&#x3D;&#39;APPLICATION_AUDIT&#39;&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li>FILESIZE</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    用于指定单个导出的数据文件的最大值，与％U一起使用。比如，我们需要导出100G的数据，文件全部存储到一个文件内，在文件传输时，会耗费大量的时间，此时我们就可以使用这个参数，限制每个文件的大小，在传输导出文件时，就可以多个文件同时传送，大大的节省了文件传输时间。提高了工作的效率。语法  FILESIZE&#x3D;integer[B | K | M | G]示例   Expdp scott&#x2F;tiger DIRECTORY&#x3D;dump_dir DUMPFILE&#x3D;halberd%U.dup FILESIZE&#x3D;20g<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li>FLASHBACK_SCN／FLASHBACK_TIME</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    基于undo 及scn号(时间点)进行的数据导出。使用此参数设置会进行flashback query的功能，查询到对应指定的SCN时的数据，然后进行导出。只要UNDO不被覆盖，无论数据库是否重启，都可以进行导出. flashback_time参数与flashback_scn的原理是一样的。在导出的数据里保持数据的一致性是很有必要的。这个。。我想，没谁傻忽忽的把这两个参数一起使用吧？所以我就不提醒你两个参数不可以同时使用了。语法   FLASHBACK_SCN&#x3D;scn_value   FLASHBACK_TIME 有多种设定值的格式：   flashback_time&#x3D;to_timestamp (localtimestamp)   flashback_time&#x3D;to_timestamp_tz (systimestamp)   flashback_time&#x3D;&quot;TO_TIMESTAMP (&quot;&quot;25-08-2003 14:35:00&quot;&quot;， &quot;&quot;DD-MM-YYYY HH24:MI:SS&quot;&quot;)&quot;  使用此格式可能会遇到ORA-39150错误。示例   Expdp scott&#x2F;tiger DIRECTORY&#x3D;dump_dir DUMPFILE&#x3D;halberd.dmp FLASHBACK_SCN&#x3D; 12345567789   Expdp scott&#x2F;tiger DIRECTORY&#x3D;dump_dir DUMPFILE&#x3D;halberd.dmp FLASHBACK_TIME&#x3D; to_timestamp (localtimestamp)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li>FULL</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用   指定导出内容为全库导出。这里需要特别注意的是,expdp 不能导出sys用户对象。即使是全库导出也不包含sys用户。语法   FULL&#x3D;&#123;Y | N&#125;示例   expdp \&#39;\&#x2F; as sysdba\&#39; directory&#x3D;dump_dir full&#x3D;y<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li><strong><span id = "include">INCLUDE</span></strong></li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    限制范围，指定自己想要的内容，比如要导出某个用户的某张表。语法    INCLUDE &#x3D; object_type[:name_clause],object_type[:name_clause]示例    impdp dbmon&#x2F;dbmon_123 directory&#x3D;dump_dir network_link&#x3D;zjzwb2 SCHEMAS&#x3D;AICBS remap_schema&#x3D;aicbs:aicbsb include&#x3D;table:\&quot;IN\(SELECT TABLE_NAME FROM dbmon.TABLES_TOBE_MASKED\)\&quot;  LOGFILE&#x3D;zjzwb.log transform&#x3D;segment_attributes:n    PARFILE中设置:        INCLUDE&#x3D;table:&quot;in(select table_name from dba_tables where owner&#x3D;&#39;AA&#39;)&quot;        INCLUDE&#x3D;TABLE:&quot;IN(&#39;TEST1&#39;,&#39;TEST2&#39;)&quot;    SHELL环境设置:        INCLUDE&#x3D;TABLE:\&quot;IN\(SELECT TABLE_NAME FROM DBA_TABLES WHERE OWNER&#x3D;\&#39;AA\&#39;\)\&quot;        INCLUDE&#x3D;TABLE:\&quot;IN\(\&#39;TEST1\&#39;,\&#39;TEST2\&#39;\)\&quot;说明    当导入命令在目标端发起时，select 子句所涉及的表要在源端，并且dblink 所使用的用户有访问的权限。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li>JOB_NAME</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    指定任务名，如果不指定的话，系统会默认自动命名：SYS_EXPORT_mode_nn语法    JOB_NAME&#x3D;&amp;JOB_NAME其他    查看有哪些expdp&#x2F;impdp job，可以通过dba_datapump_jobs查看，其实你通过v$session.action也可以查看到    大多与attach参数一起使用，重新进行expdp交互命令时使用。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li><strong>LOGFILE</strong></li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用： 指定导出日志名称。默认是：expdp.log语法    LOGFILE&#x3D;[DIRECTORY:]file_name   , 如果参数值里没有指定路径，会默认使用directory参数值所指向的路径。    directory ： 存储路径,    file_name :日志文件名示例    expdp scott&#x2F;tiger DIRECTORY&#x3D;dump_dir DUMPFILE&#x3D;halberd.dmp logfile&#x3D;halberd.log    impdp scott&#x2F;tiger DIRECTORY&#x3D;dump_dir DUMPFILE&#x3D;halberd.dmp logfile&#x3D;halberd.log<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li><strong>NETWORK_LINK</strong></li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    此参数只有在导入（impdp）时使用，可通过本地数据库里的db_link连接到其他数据库A，将数据库A的数据直接导入到本地数据库。中间可节省导出数据文件，传送数据文件的过程。很方便。但是要特别注意，不同版本之间可能会存在问题，比如源库为10g,目标库为11g。使用network_link参数会报错。至于 12C 与低版本之间是否有问题尚未尝试。语法    network_link&#x3D;[db_link]示例    impdp scott&#x2F;tiger DIRECTORY&#x3D;dump_dir DUMPFILE&#x3D;halberd.dmp NETWORK_LINK&#x3D;to_tjj SCHEMAS&#x3D;halberd logfile&#x3D;halberd.log<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li>NOLOGFILE</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    不写导入导出日志，这个笔者是灰常灰常滴不建议设置为“Y”滴。语法    nologfile&#x3D;[y|n]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li>PARALLEL</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    指定导出／导入时使用多少个并发，默认是1,如果是单个表则在dumpfile中使用...%U.dmp语法    parallel&#x3D;[digit]示例    expdp \&#39;\&#x2F; as sysdba\&#39; directory&#x3D;dump_dir schemas&#x3D;halberd dumpfile&#x3D;halberd%U.dmp parallel&#x3D;8 logfile&#x3D;halberd.log<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li>PARFILE</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    参数文件，这个参数文件里，存储着一些参数的设置。比如上面说过的，parallel,network_link,等。导出时，可以使用此参数，expdp&#x2F;impdp会自动读取文件中的参数设置，进行操作。语法    PARFILE&#x3D;[directory_path] file_name示例   expdp \&#39;\&#x2F; as sysdba\&#39; parfile&#x3D;halberd.par   cat halberd.par   directory&#x3D;dump_dir                             logfile&#x3D;test.log                               schemas&#x3D;test                                   query&#x3D;&quot;where create_date &gt; last_day(add_months(sysdate,-1)) and create_date &lt;&#x3D; last_day(sysdate)&quot;    transform&#x3D;segment_attributes:n                    network_link&#x3D;to_aibcrm   table_exists_action&#x3D;append                       impdp \&#39;\&#x2F; as sysdba\&#39; parfile&#x3D;test.par<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li><strong>QUERY</strong></li></ul><blockquote><pre class="line-numbers language-plsql" data-language="plsql"><code class="language-plsql">作用    此参数指定在导入导出时的限制条件，和<span class="token keyword">SQL</span>语句中的 <span class="token string">"where"</span> 语句是一样儿一样儿滴语法    QUERY<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">[</span>schema<span class="token punctuation">.</span><span class="token punctuation">]</span> <span class="token punctuation">[</span>table_name<span class="token operator">:</span><span class="token punctuation">]</span> query_clause<span class="token punctuation">,</span> <span class="token punctuation">[</span>schema<span class="token punctuation">.</span><span class="token punctuation">]</span> <span class="token punctuation">[</span>table_name<span class="token operator">:</span><span class="token punctuation">]</span> query_clause<span class="token punctuation">,</span>……<span class="token punctuation">)</span>    CONTENT<span class="token operator">=</span>METADATA_ONLY<span class="token punctuation">,</span> EXTIMATE_ONLY＝Y<span class="token punctuation">,</span>TRANSPORT_TABLESPACES<span class="token punctuation">.</span>示例   Expdp scott<span class="token operator">/</span>tiger <span class="token keyword">directory</span><span class="token operator">=</span>dump dumpfiel<span class="token operator">=</span><span class="token keyword">a</span><span class="token punctuation">.</span>dmp Tables<span class="token operator">=</span>emp query<span class="token operator">=</span>\"<span class="token keyword">where</span> car_buy_time <span class="token keyword">between</span> to_date<span class="token punctuation">(</span><span class="token string">'2018-01-01 0:00:00'</span><span class="token punctuation">,</span><span class="token string">'yyyy-mm-dd HH24:mi:ss'</span><span class="token punctuation">)</span> <span class="token keyword">and</span> to_date<span class="token punctuation">(</span><span class="token string">'2019-12-31 23:59:59'</span><span class="token punctuation">,</span><span class="token string">'yyyy-mm-dd HH24:mi:ss'</span><span class="token punctuation">)</span>\"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li><strong>SCHEMAS</strong></li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    指定导出／导入哪个用户语法    schemas&#x3D;schema_name[,schemaname,....]示例    expdp \&#39;\&#x2F; as sysdba\&#39; directory&#x3D;dump_dir schemas&#x3D;halberd<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li>REMAP_SCHEMA</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none"> 只在导入时使用作用    当把用户A的对象导入到用户（其实应该叫schema，将就看吧）B时，使用此参数，可实现要求格式    remap_schema&#x3D;schema1: schema2示例    impdp \&#39;\&#x2F; as sysdba\&#39; directory&#x3D;dump_dir dumpfile&#x3D;halberd.dmp logfile&#x3D;halberd.log remap_schema&#x3D;scott:halberd<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li><strong>TABLES</strong></li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    指定导出哪些表。格式    TABLES&#x3D;[schema.]table_name[:partition_name][,[schema.]table_name[:partition_name]]说明    Schema 表的所有者；table_name表名；partition_name分区名.可以同时导出不同用户的不同的表示例    expdp \&#39;\&#x2F; as sysdba\&#39; directory&#x3D;dump_dir tables&#x3D;emp.emp_no,emp.dept<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li><strong>REMAP_TABLE</strong><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    只有在导入时使用，用于将导入的数据表更名。用法    REMAP_TABLE&#x3D;S.a:b说明   S:表所在模式；a: 数据所在的原表空间; b： 目标表空间示例    impdp \&#39;\&#x2F; as sysdba\&#39; directory&#x3D;dump_dir tables&#x3D;emp,dept  remap_table&#x3D;C##TEST.emp:newemp<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote></li><li>TABLESPACES</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    指定导出／导入哪个表空间。语法    tablespaces&#x3D;tablespace_name[,tablespace_name,....]示例    expdp \&#39;\&#x2F; as sysdba\&#39; directory&#x3D;dump_dir tablespace&#x3D;user<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li>REMAP_TABLESPACE</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    只有在导入时使用，用于进行数据的表空间迁移。 把前一个表空间中的对象导入到冒号后面的表空间用法    remap_tablespace&#x3D;a:b说明   a: 数据所在的原表空间; b： 目标表空间示例   impdp \&#39;\&#x2F; as sysdba\&#39; directory&#x3D;dump_dir tables&#x3D;emp.dept remap_tablespace&#x3D;user:user1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li>TRANSPORT_FULL_CHECK</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">检查需要进行传输的表空间与其他不需要传输的表空间之间的信赖关系，默认为N。当设置为“Y”时，会对表空间之间的信赖关系进行检查，如A（索引表空间）信赖于B（表数据表空间），那么传输A而不传输B，则会出错，相反则不会报错。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></blockquote><ul><li>TRANSPORT_TABLESPACES</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用    列出需要进行数据传输的表空间格式     TRANSPORT_TABLESPACES＝tablespace1[,tablespace2,.............]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li>TRANSFORM</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用  此参数只在导入时使用，是一个用于设定存储相关的参数,有时候也是相当方便的。假如数据对应的表空间都存在的话，就根本用不到这个参数，但是，假如数据存储的表空间不存在，使用此参数导入到用户默认表空间就可以了。更灵活的，可以使用remap_tablespace参数来指定。格式    transform&#x3D;transform_name:value[bject_type]    transform_name &#x3D; [OID | PCTSPACE | SEGMENT_ATTRIBUTES | STORAGE]:[Y|N]    segment attributes:段属性包括物理属性、存储属性、表空间和日志,Y 值按照导出时的存储属性导入，N时按照用户、表的默认属性导入    storage:默认为Y，只取对象的存储属性作为导入作业的一部分    oid:  owner_id,如果指定oid&#x3D;Y(默认)，则在导入过程中将分配一个新的oid给对象表，这个参数我们基本不用管。    pctspace:通过提供一个正数作为该转换的值，可以增加对象的分配尺寸，并且数据文件尺寸等于pctspace的值(按百分比)示例    transform&#x3D;segment_attributes:n --表示将用户所有对象创建到用户默认表空间，而不再考虑原来的存储属性。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li><strong>VERSION</strong></li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">此参数主要在跨版本之间进行导数据时使用，更具体一点，是在从高版本数据库导入到低版本数据库时使用,从低版本导入到高版本，这个参数是不可用的。默认值是:compatible。此参数基本在导出时使用，导入时基本不可用。VERSION&#x3D;&#123;COMPATIBLE | LATEST | version_string&#125;COMPATIBLE       ： 以参数compatible的值为准，可以通过show parameter 查看compatible参数的值LATEST           ： 以数据库版本为准version_string   ： 指定版本。如： version&#x3D;10.2.0.1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li>SAMPLE</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">SAMPLE 抽样 给出导出表数据的百分比，参数值可以取.000001~100（不包括100）。不过导出过程不会和这里给出的百分比一样精确，是一个近似值。 格式： SAMPLE&#x3D;[[schema_name.]table_name:]sample_percent 示例： SAMPLE&#x3D;&quot;HR&quot;.&quot;EMPLOYEES&quot;:50<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></blockquote><ul><li><span id="table_exists_action">table_exists_action</span></li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">此参数只在导入时使用。作用：导入时，假如目标库中已存在对应的表，对于这种情况，提供三种不同的处理方式：append,truncate,skip,replace格式： table_exists_action&#x3D;［append | replace| skip |truncate］说明： append :   追加数据到表中       truncate:  将目标库中的同名表的数据truncate掉。       skip ：      遇到同名表，则跳过，不进行处理，注意：使用此参数值时，与该表相关的所有操作都会skip掉。       replace:    导入过程中，遇到同名表，则替换到目标库的那张表（先drop,再创建）。示例：  table_exists_action&#x3D;replace<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li>SQLFILE</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">只在导入时使用！作用： 使用此参数时，主要是将DMP文件中的metadata语句取出到一个单独的SQLfile中，而数据并不导入到数据库中格式： sqlfile&#x3D;&amp;file_name.sql示例： impdp \&#39;\&#x2F; as sysdba\&#39; directory&#x3D;dump_dir dumpfile&#x3D;halberd.dmp logfile&#x3D;halberd.log sqlfile&#x3D;halberd.sql<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></blockquote><ul><li><p>legacy mode </p><p>在11g中，才有这种模式。这种模式里兼容了以前版本中的部分参数，如：consistent,reuse_dumpfiles等（其实我现在也就知道这两个参数，哈哈，以后再遇到再补充）</p></li><li><p>consistent</p></li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">这个是保持数据一致性的一个参数。在11g中使用时，如果设置 consistent&#x3D;true,则会默认转换成 flashback_time参数，时间设置为命令开始执行的那个时间点。格式： consistent&#x3D;[true|false]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></blockquote><ul><li>reuse_dumpfiles</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">作用：重用导出的dmp文件 。假如第一次我们导失败了，虽然导出失败，但是dmp文件 还 是会生成的。在修改导出命令，第二次执行时，就可以 加上这个参数。格式： reuse_dumpfile&#x3D;[true|false]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></blockquote><ul><li>partition_options</li></ul><blockquote><pre class="line-numbers language-none"><code class="language-none">1 NONE 不对分区做特殊处理。在系统上的分区表一样创建。2 DEPARTITION 每个分区表和子分区表作为一个独立的表创建，名字使用表和分区（子分区）名字的组合。3 MERGE 将所有分区合并到一个表 注意：如果导出时使用了TRANSPORTABLE参数，这里就不能使用NONE和MERGE<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></blockquote><h2 id="交互式命令"><a href="#交互式命令" class="headerlink" title="交互式命令"></a><span id="jiaohu">交互式命令</span></h2><ol><li><p>连接到对应的job: </p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql">impdp username<span class="token operator">/</span>password attach<span class="token operator">=</span><span class="token operator">&amp;</span>job_nameexpdp username<span class="token operator">/</span>password attach<span class="token operator">=</span><span class="token operator">&amp;</span>job_name<span class="token comment">--jobname可以在运行任务时指定或者使用sql：</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> DBA_DATAPUMP_JOBS<span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>查看运行状态： status</p></li><li><p>停止导入导出： kill_job(直接kill 掉进程，不自动退出交互模式)</p></li><li><p>停止导入导出：stop_job（逐一停止job进程的运行，并退出交互模式）</p></li><li><p>修改并发值： parallel</p></li><li><p>退出交互模式： exit &#x2F; exit_client(退出到日志模式，对job无影响)</p></li></ol><h2 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h2><h3 id="不生成文件直接导入目标数据库"><a href="#不生成文件直接导入目标数据库" class="headerlink" title="不生成文件直接导入目标数据库"></a>不生成文件直接导入目标数据库</h3><ul><li>创建public dblink；</li></ul><pre class="line-numbers language-plsql" data-language="plsql"><code class="language-plsql"><span class="token keyword">CREATE</span> <span class="token keyword">PUBLIC</span> DATABASE LINK <span class="token operator">&lt;</span>pub_link_test1<span class="token operator">></span><span class="token keyword">CONNECT</span> <span class="token keyword">TO</span> <span class="token operator">&lt;</span>username<span class="token operator">></span> <span class="token keyword">IDENTIFIED</span> <span class="token keyword">BY</span> <span class="token operator">&lt;</span><span class="token string">"密码"</span><span class="token operator">></span><span class="token keyword">USING</span><span class="token string">'(DESCRIPTION =    (ADDRESS_LIST =      (ADDRESS = (PROTOCOL = TCP)(HOST = &lt;localhost>)(PORT = 1521))    )    (CONNECT_DATA =    (SERVER = DEDICATED)      (SERVICE_NAME = ORCL)    ))'</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>对DIRECTORY对应目录有<code> </code> <code>READ</code> &#x2F; <code>WRITE</code> &#x2F; <code>UTL_FILE</code> 权限；</li></ul><pre class="line-numbers language-plsql" data-language="plsql"><code class="language-plsql"><span class="token keyword">CREATE</span> <span class="token keyword">DIRECTORY</span> <span class="token operator">&lt;</span>dp_name<span class="token operator">></span> <span class="token keyword">AS</span> <span class="token operator">&lt;</span><span class="token string">'E:\dptest\test1'</span><span class="token operator">></span><span class="token punctuation">;</span><span class="token keyword">GRANT</span> <span class="token keyword">READ</span><span class="token punctuation">,</span><span class="token keyword">WRITE</span> <span class="token keyword">ON</span> <span class="token keyword">DIRECTORY</span> <span class="token operator">&lt;</span>dp_name<span class="token operator">></span> <span class="token keyword">TO</span> <span class="token operator">&lt;</span>username<span class="token operator">></span><span class="token punctuation">;</span><span class="token keyword">GRANT</span> <span class="token keyword">EXECUTE</span> <span class="token keyword">ON</span> SYS<span class="token punctuation">.</span>UTL_FILE <span class="token keyword">TO</span> <span class="token operator">&lt;</span>username<span class="token operator">></span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>导入到数据库；</li></ul><pre class="line-numbers language-plsql" data-language="plsql"><code class="language-plsql">impdp <span class="token operator">&lt;</span>username<span class="token operator">></span><span class="token operator">/</span><span class="token operator">&lt;</span>password<span class="token operator">></span><span class="token variable">@orcl</span> <span class="token keyword">directory</span><span class="token operator">=</span><span class="token operator">&lt;</span>dp_name<span class="token operator">></span> tables<span class="token operator">=</span><span class="token operator">&lt;</span><span class="token keyword">C</span>##TEST<span class="token punctuation">.</span>testcar要导入的表<span class="token operator">></span> parallel<span class="token operator">=</span><span class="token number">10</span> network_link<span class="token operator">=</span><span class="token operator">&lt;</span>pub_link_test1<span class="token operator">></span> remap_table<span class="token operator">=</span><span class="token operator">&lt;</span><span class="token keyword">C</span>##TEST<span class="token punctuation">.</span>testcar<span class="token operator">:</span>testfullcar改名<span class="token operator">></span> LOGFILE<span class="token operator">=</span>impdp<span class="token punctuation">.</span>log<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>个人测试分开导入导出共花费100min,直接导入数据库40min，速度直接提高2.5倍！</p><ul><li>注意：</li></ul><ol><li>在B服务器数据库创建到A服务器数据库的public db link；</li><li>在system下创建Directory，并赋予其读写权限，同时赋予SYS.UTL_FILE的执行权限；</li><li>执行脚本参数位置。</li></ol><h3 id="导入多张表"><a href="#导入多张表" class="headerlink" title="导入多张表"></a>导入多张表</h3><p>使用**<a href="#include">INCLUDE</a>**参数</p><h3 id="如果存在表将导入文件追加到文件尾-x2F-跳过-x2F-删除"><a href="#如果存在表将导入文件追加到文件尾-x2F-跳过-x2F-删除" class="headerlink" title="如果存在表将导入文件追加到文件尾&#x2F;跳过&#x2F;删除"></a>如果存在表将导入文件追加到文件尾&#x2F;跳过&#x2F;删除</h3><p>使用**<a href="#table_exists_action">table_exists_action</a>**参数：［append | replace| skip |truncate］</p><h3 id="查看进度"><a href="#查看进度" class="headerlink" title="查看进度"></a>查看进度</h3><p>在<a href="#jiaohu">交互式命令</a>里面status可以查看导入百分比等详细信息，导出没有显示百分比；</p><h3 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h3><h4 id="1、ORA-39112"><a href="#1、ORA-39112" class="headerlink" title="1、ORA-39112"></a>1、ORA-39112</h4><p>导出正常，导入数据时，只成功导入部分记录等数据，另外的部分提数ora 39112错误，经查是因为导出的用户数据中，有部分记录的表用的索引在另一表空间中，该空间还未创建，所以导致该失败。</p><p>解决方法：在导入时，添加参数：RANSFORM&#x3D;segment_attributes:n ，配合table_exists_action&#x3D;replace参数，重新导入即可。</p><p>RANSFORM&#x3D;segment_attributes:n 在导入时，会将数据导入默认的表空间中。</p><p> 补充，造成该问题的可能原因：</p><pre class="line-numbers language-none"><code class="language-none">1、在原来测试库中，目标schema和别的用户相互授权了，可是你导出的dmp中没有包含所有的用户，导入时对应用户没有创建。2、再就是，表空间问题，测试库中的用户下的某个表的索引没有在他的默认表空间里，这样你要在目标端（这里就是生产环境），创建好对应的表空间，就是说如果你在测试库把a用户的下的某个表的权限授给了b,那么你在把a用户用数据泵倒进生产库时，他会在生产库中检测有没有用户ｂ。也要做相同的操作。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h4 id="2、ORA-39346"><a href="#2、ORA-39346" class="headerlink" title="2、ORA-39346"></a>2、ORA-39346</h4><p>导入过程中，遇到错误”  ORA-39346: data loss in character set conversion for object SCHEMA_EXPORT&#x2F;PROCEDURE&#x2F;PROCEDURE  “</p><p>oracle官方的描述如下：</p><ul><li><strong>Description:</strong> data loss in character set conversion for object string</li><li><strong>Cause:</strong> Oracle Data Pump import converted a metadata object from the export database character set into the target database character set prior to processing the object. Some characters could not be converted to the target database character set and so the default replacement character was used.</li><li><strong>Action:</strong> No specific user action is required. This type of data loss can occur if the target database character set is not a superset of the export databases character set.</li></ul><p> 翻译：不需要特定的用户操作。如果目标数据库字符集不是导出数据库字符集的超集，则可能发生此类数据丢失。</p><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><ul><li>expdp和impdp是服务端的工具程序，他们只能在Oracle服务端使用，不能在客户端使用。</li><li>impdp&#x2F;expdp组合使用，不可以与exp&#x2F;imp混用。</li><li>导数据之间的数据库字符集必须一致，否则可能出现乱码。</li><li>经过测试一次只能有一个job在运行</li></ul><h2 id="脚本（转自）"><a href="#脚本（转自）" class="headerlink" title="脚本（转自）"></a>脚本（<a href="https://www.cnblogs.com/chinas/p/8300955.html#_label5">转自</a>）</h2><span id="more"></span><pre class="line-numbers language-sh" data-language="sh"><code class="language-sh">#!&#x2F;bin&#x2F;bash##############################################################################脚本功能：#脚本路径：#使用方法：脚本名称 操作类型参数#############################################################################export NLS_LANG&#x3D;american_america.AL32UTF8export ORACLE_HOME&#x3D;&#x2F;u01&#x2F;app&#x2F;oracle&#x2F;product&#x2F;12.1.0&#x2F;db_1export ORACLE_SID&#x3D;cyrtestdbexport PATH&#x3D;$PATH:$ORACLE_HOME&#x2F;bin:&#x2F;usr&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;bin:&#x2F;usr&#x2F;bin&#x2F;X11:&#x2F;usr&#x2F;local&#x2F;bin:.v_date&#x3D;&#96;date +%Y%m%d&#96;v_logfile&#x3D;&#96;echo $(basename  $0) | awk -F &quot;.&quot; &#39;&#123;print $1&quot;.log&quot;&#125;&#39;&#96;  #日志文件名称：脚本所在目录下，脚本名称.logv_usr_1&#x3D;&quot;&quot;            #Oracle用户名、密码v_pwd_1&#x3D;&quot;&quot;v_usr_2&#x3D;&quot;&quot;v_pwd_2&#x3D;&quot;&quot;            v_db_instance&#x3D;&quot;&quot;    #数据库实例名v_backup_dir&#x3D;&quot;&quot;        #Oracle备份目录(全局变量)v_oradir_name&#x3D;&quot;&quot;    #v_tmp_space1&#x3D;&quot;&quot;        #临时表空间、表空间、索引表空间v_tmp_space2&#x3D;&quot;&quot;v_space1&#x3D;&quot;&quot;v_space2&#x3D;&quot;&quot;v_idx_space1&#x3D;&quot;&quot;v_idx_space2&#x3D;&quot;&quot;v_max_size&#x3D;&quot;5120m&quot;    #表空间数据文件最大值v_dblink&#x3D;&quot;&quot;            #dblink名称#记录日志record_log()&#123;    echo -e &#96;date &#39;+%Y-%m-%d %H:%M:%S&#39;&#96; $1 | tee -a $&#123;v_logfile&#125;&#125;#用户数据导出exp_usrdata()&#123;    v_exp_usr&#x3D;$1    v_exp_pwd&#x3D;$2    v_oradir_name&#x3D;$3    cd $&#123;v_backup_dir&#125;    [[ -f $&#123;v_exp_usr&#125;&quot;_&quot;$&#123;v_date&#125;&quot;.dmp&quot; ]] &amp;&amp; rm -rf $&#123;v_exp_usr&#125;&quot;_&quot;$&#123;v_date&#125;&quot;.dmp&quot;    expdp $&#123;v_exp_usr&#125;&#x2F;$&#123;v_exp_pwd&#125; DIRECTORY&#x3D;$&#123;v_oradir_name&#125; DUMPFILE&#x3D;$&#123;v_exp_usr&#125;&quot;_&quot;$&#123;v_date&#125;&quot;.dmp&quot; SCHEMAS&#x3D;$&#123;v_exp_usr&#125; LOGFILE&#x3D;$&#123;v_exp_usr&#125;&quot;_&quot;$&#123;v_date&#125;&quot;.log&quot;&#125;#在目标库上创建数据库备份目录create_bankup_dir()&#123;    #创建操作系统物理路径    [[ -d $&#123;v_backup_dir&#125; ]] &amp;&amp; mkdir -p $&#123;v_backup_dir&#125;    sqlplus -S &#x2F; as sysdba &gt;&gt; $&#123;v_logfile&#125; &lt;&lt;EOF    set heading off feedback off verify off    spool tmp_space_flag.tmp    grant read,write on directory &#39;$&#123;v_oradir_name&#125;&#39; to &#39;$&#123;v_usr_1&#125;&#39;,&#39;$&#123;v_usr_2&#125;&#39;;    spool off    exit;EOF    ##如果当前表空间不存在，则创建，否则退出当前函数    if [[ &#96;grep $&#123;v_oradir_name&#125; tmp_space_flag.tmp | wc -l&#96; -eq 0 ]]; then        record_log &quot;创建备份目录&quot;$1&quot;开始&quot;        sqlplus -S &#x2F; as sysdba &gt;&gt; $&#123;v_logfile&#125; &lt;&lt;EOF        set heading off feedback off verify off        grant read,write on directory &#39;$&#123;v_oradir_name&#125;&#39; to &#39;$&#123;v_usr_1&#125;&#39;,&#39;$&#123;v_usr_2&#125;&#39;;        exit;EOF        record_log &quot;创建备份目录&quot;$1&quot;结束&quot;    else        record_log &quot;创建备份目录&quot;$1&quot;已存在&quot;        return    fi    ##注意清理临时标志文件    [[ -f .&#x2F;tmp_space_flag.tmp ]] &amp;&amp; rm -rf tmp_space_flag.tmp&#125;#创建表空间create_space()&#123;    v_space_name&#x3D;$1    #判断表空间类型（临时表空间或普通表空间）    if [[ &#96;grep TMP $1&#96; -eq 1 ]]; then        v_space_type&#x3D;&quot;temporary&quot;    else        v_space_type&#x3D;&quot;&quot;    fi     sqlplus -S &#x2F; as sysdba &gt;&gt; $&#123;v_logfile&#125; &lt;&lt;EOF    set heading off feedback off verify off    create &#39;$&#123;v_space_type&#125;&#39; tablespace &#39;$&#123;v_space_name&#125;&#39; tempfile &#39;$&#123;v_backup_dir&#125;&#x2F;$&#123;v_space_name&#125;.dbf&#39; size 50m autoextend on next 128k maxsize &#39;$&#123;v_max_size&#125;&#39; extent management local;     exit;EOF&#125;#判断表空间是否存在，若表空间不存在则创建deal_spaces()&#123;    v_space_name&#x3D;$1    sqlplus -S &#x2F; as sysdba &lt;&lt;EOF     set heading off feedback off verify off    spool tmp_space_flag.tmp    select tablespace_name from dba_tablespaces where tablespace_name&#x3D;&#39;$&#123;v_space_name&#125;&#39;;    spool off    exit;EOF    ##如果当前表空间不存在，则创建，否则退出当前函数    if [[ &#96;grep $&#123;v_space_name&#125; tmp_space_flag.tmp | wc -l&#96; -eq 0 ]]; then        record_log &quot;创建表空间&quot;$1&quot;开始&quot;        create_space $&#123;v_space_name&#125;        record_log &quot;创建表空间&quot;$1&quot;结束&quot;    else        record_log &quot;表空间&quot;$1&quot;已存在&quot;        return    fi    ##注意清理临时标志文件    [[ -f .&#x2F;tmp_space_flag.tmp ]] &amp;&amp; rm -rf tmp_space_flag.tmp&#125;#在目标库上创建用户并赋权create_usrs()&#123;    v_create_usr&#x3D;$1        #参数1：用户名    v_create_pwd&#x3D;$2        #参数2：密码    v_create_tmp_space&#x3D;$3         #参数3：临时表空间名称    v_create_space&#x3D;$4        #参数4：表空间名称    sqlplus -S &#x2F; as sysdba &gt;&gt; $&#123;v_logfile&#125; &lt;&lt;EOF    set heading off feedback off verify off    create user &#39;$&#123;v_create_usr&#125;&#39; identified by &#39;$&#123;v_create_pwd&#125;&#39; default tablespace &#39;$&#123;v_create_space&#125;&#39; temporary tablespace &#39;$&#123;v_create_tmp_space&#125;&#39;;    grant connect,resource to &#39;$&#123;v_create_usr&#125;&#39;;    grant exp_full_database to &#39;$&#123;v_create_usr&#125;&#39;;    grant imp_full_database to &#39;$&#123;v_create_usr&#125;&#39;;    grant unlimited tablespace to &#39;$&#123;v_create_usr&#125;&#39;;    exit;EOF&#125;#用户数据导入imp_usrdata()&#123;    v_imp_usr&#x3D;$1    v_imp_pwd&#x3D;$2    v_oradir_name&#x3D;$3    impdp $&#123;v_imp_usr&#125;&#x2F;$&#123;v_imp_pwd&#125; DIRECTORY&#x3D;$&#123;v_oradir_name&#125; DUMPFILE&#x3D;$&#123;v_imp_usr&#125;&quot;_&quot;$&#123;v_date&#125;&quot;.dmp&quot; SCHEMAS&#x3D;$&#123;v_imp_usr&#125; LOGFILE&#x3D;$&#123;v_imp_usr&#125;&quot;_&quot;$&#123;v_date&#125;&quot;.log&quot; table_exists_action&#x3D;replace RANSFORM&#x3D;segment_attributes:n&#125;#删除用户drop_user()&#123;    v_drop_usr&#x3D;$1    #删除用户及用户下的所有数据，删除表空间及表空间下的所有数据    sqlplus -S &#x2F; as sysdba &gt;&gt; $&#123;v_logfile&#125; &lt;&lt;EOF    set heading off feedback off verify off    drop user &#39;$&#123;v_drop_usr&#125;&#39; cascade;    exitEOF&#125;#删除表空间并删除表空间下的数据文件drop_tablespace()&#123;    v_drop_space&#x3D;$1    #删除用户及用户下的所有数据，删除表空间及表空间下的所有数据    sqlplus -S &#x2F; as sysdba &gt;&gt; $&#123;v_logfile&#125; &lt;&lt;EOF    set heading off feedback off verify off    drop tablespace &#39;$&#123;v_drop_space&#125;&#39; including contents and datafiles;    exitEOF    ##操作系统上表空间下的数据文件    [[ -f $&#123;v_backup_dir&#125;&#x2F;$&#123;v_drop_space&#125;.dbf ]] &amp;&amp; rm -rf $&#123;v_backup_dir&#125;&#x2F;$&#123;v_drop_space&#125;.dbf&#125;#创建dblinkcreate_dblink()&#123;    v_clink_usr&#x3D;$1    v_clink_pwd&#x3D;$2    #以管理员身份对创建dblink赋权    sqlplus -S &#x2F; as sysdba &gt;&gt; $&#123;v_logfile&#125; &lt;&lt;EOF    set heading off feedback off verify off    grant create database link to &#39;$&#123;v_clink_usr&#125;&#39;;    exit;EOF    #以普通用户登录创建dblink    sqlplus -S $&#123;v_clink_usr&#125;&#x2F;$&#123;v_clink_pwd&#125;@$&#123;v_db_instance&#125; &gt;&gt; $&#123;v_logfile&#125; &lt;&lt;EOF    set heading off feedback off verify off    CREATE DATABASE LINK $&#123;v_dblink&#125; CONNECT TO &#39;$&#123;v_clink_usr&#125;&#39; IDENTIFIED BY &#39;$&#123;v_clink_pwd&#125;&#39; USING &#39;&#123;v_db_instance&#125;&#39;;    exitEOF&#125;#判断dblink是否存在deal_dblink()&#123;    v_link&#x3D;$1    v_link_usr&#x3D;$2    v_link_pwd&#x3D;$3    sqlplus -S &#x2F; as sysdba &lt;&lt;EOF     set heading off feedback off verify off    spool tmp_space_flag.tmp    select object_name from dba_objects where object_name &#x3D; &#39;$&#123;v_link&#125;&#39;;     spool off    exit;EOF    ##如果当前dblink不存在，则创建，否则退出当前函数    if [[ &#96;grep $&#123;v_space_name&#125; tmp_space_flag.tmp | wc -l&#96; -eq 0 ]]; then        record_log &quot;创建&quot;$1&quot;开始&quot;        create_dblink $&#123;v_link_usr&#125; $&#123;v_link_pwd&#125;        record_log &quot;创建&quot;$1&quot;结束&quot;    else        record_log $1&quot;已存在&quot;        return    fi    ##注意清理临时标志文件    [[ -f .&#x2F;tmp_space_flag.tmp ]] &amp;&amp; rm -rf tmp_space_flag.tmp&#125;#主函数main()&#123;    v_start&#x3D;&#96;date +%s&#96;    if [[ $1 -eq &quot;exp&quot; ]]; then         record_log &quot;bl库导出开始...&quot;        exp_usrdata $&#123;v_usr_1&#125; $&#123;v_pwd_1&#125; $&#123;v_oradir_name&#125;        record_log &quot;bl库导出结束...&quot;        record_log &quot;hx库导出开始...&quot;        exp_usrdata $&#123;v_usr_2&#125; $&#123;v_pwd_2&#125; $&#123;v_oradir_name&#125;        record_log &quot;hx库导出结束...&quot;    elif [[ $1 -eq &quot;pre&quot; ]]; then        #1、创建备份目录        create_bankup_dir        #2、创建表空间        for v_sp in $&#123;v_tmp_space1&#125; $&#123;v_tmp_space2&#125; $&#123;v_space1&#125; $&#123;v_space2&#125; $&#123;v_idx_space1&#125; $&#123;v_idx_space2&#125;; do            deal_spaces $&#123;v_sp&#125;        done        #3、创建用户、赋权        record_log &quot;创建用户开始...&quot;        create_usrs $&#123;v_usr_1&#125; $&#123;v_pwd_1&#125; $&#123;v_tmp_space1&#125; $&#123;v_space1&#125;        create_usrs $&#123;v_usr_2&#125; $&#123;v_pwd_2&#125; $&#123;v_tmp_space2&#125; $&#123;v_space2&#125;        record_log &quot;创建用户结束...&quot;        #4、为hx库创建dblink        record_log &quot;创建dblink开始...&quot;        deal_dblink $&#123;v_dblink&#125; $&#123;v_usr_2&#125; $&#123;v_pwd_2&#125;        record_log &quot;创建dblink结束...&quot;    elif [[ $1 -eq &quot;imp&quot; ]]; then        record_log &quot;bl库导入开始...&quot;        imp_usrdata $&#123;v_usr_1&#125; $&#123;v_pwd_1&#125; $&#123;v_oradir_name&#125;        record_log &quot;bl库导入结束...&quot;        record_log &quot;hx库导入开始...&quot;        imp_usrdata $&#123;v_usr_2&#125; $&#123;v_pwd_2&#125; $&#123;v_oradir_name&#125;        record_log &quot;hx库导入结束...&quot;    elif [[ $1 -eq &quot;clean&quot; ]]; then        read -t 5 -p &quot;确认清除 0-否 1-是&quot; v_num        if [[ $&#123;v_num&#125; -eq 1 ]]; then            record_log &quot;清理数据文件开始...&quot;            for m in $&#123;&#125; $&#123;&#125;; do                drop_user $&#123;m&#125;            done            for n in $&#123;v_tmp_space1&#125; $&#123;v_tmp_space2&#125; $&#123;v_space1&#125; $&#123;v_space2&#125; $&#123;v_idx_space1&#125; $&#123;v_idx_space2&#125;; do                drop_tablespace $&#123;n&#125;            done            record_log &quot;清理数据文件结束...&quot;        else            exit        fi    else        echo &quot;Usage: sh script [exp|clean|pre|imp]&quot;        exit    fi    v_end&#x3D;&#96;date +%s&#96;    v_use_time&#x3D;$[ v_end - v_start ]    record_log &quot;本次脚本运行时间：&quot;$&#123;v_use_time&#125;&quot;秒&quot;&#125;main<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Oracle </category>
          
      </categories>
      
      
        <tags>
            
            <tag> oracle </tag>
            
            <tag> 数据迁移 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
